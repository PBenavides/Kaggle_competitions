{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as ms\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "data = pd.read_excel('DataJunio_19.xlsx') #No preocuparse por el tamaño, pesa 13 megas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AÑO</th>\n",
       "      <th>MES</th>\n",
       "      <th>FOB (US$)</th>\n",
       "      <th>PESO NETO (KG)</th>\n",
       "      <th>NUMERO DE DOCUMENTO</th>\n",
       "      <th>NOMBRE/RAZON SOCIAL</th>\n",
       "      <th>PAIS</th>\n",
       "      <th>SUBPARTIDA</th>\n",
       "      <th>SUBPARTIDA-DESCRIPCION</th>\n",
       "      <th>TIPO SECTOR</th>\n",
       "      <th>SECTOR SUNAT</th>\n",
       "      <th>MACRO SECTOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>110.00</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0000022B</td>\n",
       "      <td>EMB. ESTADOS UNIDOS-NAS-ASUNTOS ANTINARCOTICOS</td>\n",
       "      <td>Estados Unidos</td>\n",
       "      <td>4901999000</td>\n",
       "      <td>4901999000 - LOS DEMÁS LIBROS, FOLLETOS E IMPR...</td>\n",
       "      <td>NO TRADICIONAL</td>\n",
       "      <td>MADERAS Y PAPELES</td>\n",
       "      <td>NO DEFINIDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1144.74</td>\n",
       "      <td>1.676</td>\n",
       "      <td>0000022B</td>\n",
       "      <td>EMB. ESTADOS UNIDOS-NAS-ASUNTOS ANTINARCOTICOS</td>\n",
       "      <td>Estados Unidos</td>\n",
       "      <td>8204110000</td>\n",
       "      <td>8204110000 - LLAVES DE AJUSTE DE MANO DE BOCA ...</td>\n",
       "      <td>NO TRADICIONAL</td>\n",
       "      <td>METAL-MECÁNICO</td>\n",
       "      <td>NO DEFINIDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>3473.00</td>\n",
       "      <td>23.750</td>\n",
       "      <td>0000022B</td>\n",
       "      <td>EMB. ESTADOS UNIDOS-NAS-ASUNTOS ANTINARCOTICOS</td>\n",
       "      <td>Estados Unidos</td>\n",
       "      <td>8409100000</td>\n",
       "      <td>8409100000 - PARTES DE MOTORES DE AVIACIÓN, PA...</td>\n",
       "      <td>NO TRADICIONAL</td>\n",
       "      <td>METAL-MECÁNICO</td>\n",
       "      <td>NO DEFINIDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>810.00</td>\n",
       "      <td>1.052</td>\n",
       "      <td>0000022B</td>\n",
       "      <td>EMB. ESTADOS UNIDOS-NAS-ASUNTOS ANTINARCOTICOS</td>\n",
       "      <td>Estados Unidos</td>\n",
       "      <td>8409917000</td>\n",
       "      <td>8409917000 - VÁLVULAS PARA MOTORES DE LA PARTI...</td>\n",
       "      <td>NO TRADICIONAL</td>\n",
       "      <td>METAL-MECÁNICO</td>\n",
       "      <td>NO DEFINIDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1573.79</td>\n",
       "      <td>2.833</td>\n",
       "      <td>0000022B</td>\n",
       "      <td>EMB. ESTADOS UNIDOS-NAS-ASUNTOS ANTINARCOTICOS</td>\n",
       "      <td>Estados Unidos</td>\n",
       "      <td>8423829090</td>\n",
       "      <td>8423829090 - LOS DEMÁS APARATOS PARA PESAR DE ...</td>\n",
       "      <td>NO TRADICIONAL</td>\n",
       "      <td>METAL-MECÁNICO</td>\n",
       "      <td>NO DEFINIDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206248</th>\n",
       "      <td>2019</td>\n",
       "      <td>2019-06-06</td>\n",
       "      <td>57817.00</td>\n",
       "      <td>190.000</td>\n",
       "      <td>20604687561</td>\n",
       "      <td>CONE TEXTIL S.A.C.</td>\n",
       "      <td>Chile</td>\n",
       "      <td>6204440000</td>\n",
       "      <td>6204440000 - VESTIDOS, PARA MUJERES O NIÑAS, D...</td>\n",
       "      <td>NO TRADICIONAL</td>\n",
       "      <td>TEXTIL</td>\n",
       "      <td>NO DEFINIDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206249</th>\n",
       "      <td>2019</td>\n",
       "      <td>2019-06-06</td>\n",
       "      <td>47950.50</td>\n",
       "      <td>130.000</td>\n",
       "      <td>20604710970</td>\n",
       "      <td>INVERSIONES CHANDIA E.I.R.L.</td>\n",
       "      <td>Chile</td>\n",
       "      <td>6204440000</td>\n",
       "      <td>6204440000 - VESTIDOS, PARA MUJERES O NIÑAS, D...</td>\n",
       "      <td>NO TRADICIONAL</td>\n",
       "      <td>TEXTIL</td>\n",
       "      <td>NO DEFINIDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206250</th>\n",
       "      <td>2019</td>\n",
       "      <td>2019-06-06</td>\n",
       "      <td>32686.00</td>\n",
       "      <td>140.000</td>\n",
       "      <td>20604710970</td>\n",
       "      <td>INVERSIONES CHANDIA E.I.R.L.</td>\n",
       "      <td>Chile</td>\n",
       "      <td>6206400000</td>\n",
       "      <td>6206400000 - CAMISAS, BLUSAS Y BLUSAS CAMISERA...</td>\n",
       "      <td>NO TRADICIONAL</td>\n",
       "      <td>TEXTIL</td>\n",
       "      <td>NO DEFINIDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206251</th>\n",
       "      <td>2019</td>\n",
       "      <td>2019-06-06</td>\n",
       "      <td>184573.00</td>\n",
       "      <td>910.000</td>\n",
       "      <td>20604715670</td>\n",
       "      <td>GRUPO TEXTIL BERROCAL E.I.R.L.</td>\n",
       "      <td>Chile</td>\n",
       "      <td>8308900000</td>\n",
       "      <td>8308900000 - CIERRES, Y ARTÍCULOS. SIMILARES. ...</td>\n",
       "      <td>NO TRADICIONAL</td>\n",
       "      <td>METAL-MECÁNICO</td>\n",
       "      <td>NO DEFINIDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206252</th>\n",
       "      <td>2019</td>\n",
       "      <td>2019-06-06</td>\n",
       "      <td>26000.00</td>\n",
       "      <td>1725.000</td>\n",
       "      <td>910279140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Estados Unidos</td>\n",
       "      <td>8703239020</td>\n",
       "      <td>8703239020 - LOS DEMÁS ENSAMBLADOS</td>\n",
       "      <td>NO TRADICIONAL</td>\n",
       "      <td>METAL-MECÁNICO</td>\n",
       "      <td>NO DEFINIDO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198370 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AÑO        MES  FOB (US$)  PESO NETO (KG) NUMERO DE DOCUMENTO  \\\n",
       "0       2018 2019-01-01     110.00           0.182            0000022B   \n",
       "1       2018 2019-01-01    1144.74           1.676            0000022B   \n",
       "2       2018 2019-01-01    3473.00          23.750            0000022B   \n",
       "3       2018 2019-01-01     810.00           1.052            0000022B   \n",
       "4       2018 2019-01-01    1573.79           2.833            0000022B   \n",
       "...      ...        ...        ...             ...                 ...   \n",
       "206248  2019 2019-06-06   57817.00         190.000         20604687561   \n",
       "206249  2019 2019-06-06   47950.50         130.000         20604710970   \n",
       "206250  2019 2019-06-06   32686.00         140.000         20604710970   \n",
       "206251  2019 2019-06-06  184573.00         910.000         20604715670   \n",
       "206252  2019 2019-06-06   26000.00        1725.000           910279140   \n",
       "\n",
       "                                   NOMBRE/RAZON SOCIAL            PAIS  \\\n",
       "0       EMB. ESTADOS UNIDOS-NAS-ASUNTOS ANTINARCOTICOS  Estados Unidos   \n",
       "1       EMB. ESTADOS UNIDOS-NAS-ASUNTOS ANTINARCOTICOS  Estados Unidos   \n",
       "2       EMB. ESTADOS UNIDOS-NAS-ASUNTOS ANTINARCOTICOS  Estados Unidos   \n",
       "3       EMB. ESTADOS UNIDOS-NAS-ASUNTOS ANTINARCOTICOS  Estados Unidos   \n",
       "4       EMB. ESTADOS UNIDOS-NAS-ASUNTOS ANTINARCOTICOS  Estados Unidos   \n",
       "...                                                ...             ...   \n",
       "206248                              CONE TEXTIL S.A.C.           Chile   \n",
       "206249                    INVERSIONES CHANDIA E.I.R.L.           Chile   \n",
       "206250                    INVERSIONES CHANDIA E.I.R.L.           Chile   \n",
       "206251                  GRUPO TEXTIL BERROCAL E.I.R.L.           Chile   \n",
       "206252                                             NaN  Estados Unidos   \n",
       "\n",
       "        SUBPARTIDA                             SUBPARTIDA-DESCRIPCION  \\\n",
       "0       4901999000  4901999000 - LOS DEMÁS LIBROS, FOLLETOS E IMPR...   \n",
       "1       8204110000  8204110000 - LLAVES DE AJUSTE DE MANO DE BOCA ...   \n",
       "2       8409100000  8409100000 - PARTES DE MOTORES DE AVIACIÓN, PA...   \n",
       "3       8409917000  8409917000 - VÁLVULAS PARA MOTORES DE LA PARTI...   \n",
       "4       8423829090  8423829090 - LOS DEMÁS APARATOS PARA PESAR DE ...   \n",
       "...            ...                                                ...   \n",
       "206248  6204440000  6204440000 - VESTIDOS, PARA MUJERES O NIÑAS, D...   \n",
       "206249  6204440000  6204440000 - VESTIDOS, PARA MUJERES O NIÑAS, D...   \n",
       "206250  6206400000  6206400000 - CAMISAS, BLUSAS Y BLUSAS CAMISERA...   \n",
       "206251  8308900000  8308900000 - CIERRES, Y ARTÍCULOS. SIMILARES. ...   \n",
       "206252  8703239020                 8703239020 - LOS DEMÁS ENSAMBLADOS   \n",
       "\n",
       "           TIPO SECTOR       SECTOR SUNAT MACRO SECTOR  \n",
       "0       NO TRADICIONAL  MADERAS Y PAPELES  NO DEFINIDO  \n",
       "1       NO TRADICIONAL     METAL-MECÁNICO  NO DEFINIDO  \n",
       "2       NO TRADICIONAL     METAL-MECÁNICO  NO DEFINIDO  \n",
       "3       NO TRADICIONAL     METAL-MECÁNICO  NO DEFINIDO  \n",
       "4       NO TRADICIONAL     METAL-MECÁNICO  NO DEFINIDO  \n",
       "...                ...                ...          ...  \n",
       "206248  NO TRADICIONAL             TEXTIL  NO DEFINIDO  \n",
       "206249  NO TRADICIONAL             TEXTIL  NO DEFINIDO  \n",
       "206250  NO TRADICIONAL             TEXTIL  NO DEFINIDO  \n",
       "206251  NO TRADICIONAL     METAL-MECÁNICO  NO DEFINIDO  \n",
       "206252  NO TRADICIONAL     METAL-MECÁNICO  NO DEFINIDO  \n",
       "\n",
       "[198370 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data['TIPO SECTOR'] == 'NO TRADICIONAL']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('MACRO SECTOR',axis=1,inplace=True)\n",
    "data.drop('TIPO SECTOR',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['US$/kg'] = data['FOB (US$)'] / data['PESO NETO (KG)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198370, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La columna AÑO tiene 2 valores unicos\n",
      "La columna MES tiene 6 valores unicos\n",
      "La columna FOB (US$) tiene 135745 valores unicos\n",
      "La columna PESO NETO (KG) tiene 114071 valores unicos\n",
      "La columna NUMERO DE DOCUMENTO tiene 7586 valores unicos\n",
      "La columna NOMBRE/RAZON SOCIAL tiene 7535 valores unicos\n",
      "La columna PAIS tiene 174 valores unicos\n",
      "La columna SUBPARTIDA tiene 4676 valores unicos\n",
      "La columna SUBPARTIDA-DESCRIPCION tiene 4676 valores unicos\n",
      "La columna SECTOR SUNAT tiene 11 valores unicos\n",
      "La columna US$/kg tiene 177014 valores unicos\n"
     ]
    }
   ],
   "source": [
    "for col in data.columns:\n",
    "    print('La columna',col,'tiene {} valores unicos'.format(data[col].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FOB (US$)</th>\n",
       "      <th>PESO NETO (KG)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOMBRE/RAZON SOCIAL</th>\n",
       "      <th>MES</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">6</th>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>2570.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-02</th>\n",
       "      <td>28000.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SERVICIOS GENERALES HUZA &amp; OÑA  E.I.R.L</th>\n",
       "      <th>2019-04-04</th>\n",
       "      <td>86241.210000</td>\n",
       "      <td>57826.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">AQUINO Y CIA SOCIEDAD COMERCIAL DE RESPONSABILIDAD LIMITADA</th>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>1362.791667</td>\n",
       "      <td>160.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-03</th>\n",
       "      <td>2545.333333</td>\n",
       "      <td>368.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZUBIETA AND PARTNERS E.I.R.L</th>\n",
       "      <th>2019-06-06</th>\n",
       "      <td>1001.110000</td>\n",
       "      <td>10.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ZURI SOFT E.I.R.L.</th>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>2832.530000</td>\n",
       "      <td>46.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-03</th>\n",
       "      <td>2613.500000</td>\n",
       "      <td>29.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-04</th>\n",
       "      <td>2376.000000</td>\n",
       "      <td>31.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ÑAPU PERU COLECTION E.I.R.L.</th>\n",
       "      <th>2019-04-04</th>\n",
       "      <td>7001.500000</td>\n",
       "      <td>9.975000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22862 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  FOB (US$)  \\\n",
       "NOMBRE/RAZON SOCIAL                                MES                        \n",
       "6                                                  2019-01-01  10000.000000   \n",
       "                                                   2019-02-02  28000.000000   \n",
       "  SERVICIOS GENERALES HUZA & OÑA  E.I.R.L          2019-04-04  86241.210000   \n",
       " AQUINO Y CIA SOCIEDAD COMERCIAL DE RESPONSABIL... 2019-01-01   1362.791667   \n",
       "                                                   2019-03-03   2545.333333   \n",
       "...                                                                     ...   \n",
       "ZUBIETA AND PARTNERS E.I.R.L                       2019-06-06   1001.110000   \n",
       "ZURI SOFT E.I.R.L.                                 2019-01-01   2832.530000   \n",
       "                                                   2019-03-03   2613.500000   \n",
       "                                                   2019-04-04   2376.000000   \n",
       "ÑAPU PERU COLECTION E.I.R.L.                       2019-04-04   7001.500000   \n",
       "\n",
       "                                                               PESO NETO (KG)  \n",
       "NOMBRE/RAZON SOCIAL                                MES                         \n",
       "6                                                  2019-01-01     2570.000000  \n",
       "                                                   2019-02-02     2200.000000  \n",
       "  SERVICIOS GENERALES HUZA & OÑA  E.I.R.L          2019-04-04    57826.500000  \n",
       " AQUINO Y CIA SOCIEDAD COMERCIAL DE RESPONSABIL... 2019-01-01      160.833333  \n",
       "                                                   2019-03-03      368.000000  \n",
       "...                                                                       ...  \n",
       "ZUBIETA AND PARTNERS E.I.R.L                       2019-06-06       10.925000  \n",
       "ZURI SOFT E.I.R.L.                                 2019-01-01       46.300000  \n",
       "                                                   2019-03-03       29.300000  \n",
       "                                                   2019-04-04       31.300000  \n",
       "ÑAPU PERU COLECTION E.I.R.L.                       2019-04-04        9.975000  \n",
       "\n",
       "[22862 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['NOMBRE/RAZON SOCIAL', 'MES'])[['FOB (US$)','PESO NETO (KG)']].agg('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primero voy a separar mi train y test... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2019-01-01T00:00:00.000000000', '2019-02-02T00:00:00.000000000',\n",
       "       '2019-03-03T00:00:00.000000000', '2019-04-04T00:00:00.000000000',\n",
       "       '2019-05-05T00:00:00.000000000', '2019-06-06T00:00:00.000000000'],\n",
       "      dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Van desde enero a Junio...\n",
    "data['MES'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MES'] = pd.DatetimeIndex(data.MES).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.MES.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data[data['MES'] == 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data['MES'] != 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Hay valores que estén en test pero no estén en train? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AÑO', 'MES', 'FOB (US$)', 'PESO NETO (KG)', 'NUMERO DE DOCUMENTO',\n",
       "       'NOMBRE/RAZON SOCIAL', 'PAIS', 'SUBPARTIDA', 'SUBPARTIDA-DESCRIPCION',\n",
       "       'SECTOR SUNAT', 'US$/kg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_a_evaluar = ['NUMERO DE DOCUMENTO','NOMBRE/RAZON SOCIAL','PAIS','SUBPARTIDA','SUBPARTIDA-DESCRIPCION','SECTOR SUNAT']\n",
    "#SE HALLA QUE EN SECTOR SUNAT NO OCURREN ESTOS CASOS..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n",
      "(188, 11)\n"
     ]
    }
   ],
   "source": [
    "subpartida_lista = []\n",
    "for val_train in [*test['SUBPARTIDA'].unique()]: # De los valos que hay en test\n",
    "    if val_train not in [*train['SUBPARTIDA'].unique()]: #Cuales no están en train?\n",
    "        subpartida_lista.append(val_train)\n",
    "    else:\n",
    "        pass\n",
    "print(len(subpartida_lista))\n",
    "print(data[data['SUBPARTIDA'].isin(subpartida_lista)].shape)\n",
    "#Al parecer tienen los mismos valores para SUBPARTIDA-DESCRIPCION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LISTO(dropeado más abajo)\n",
    "pais_lista = []\n",
    "for val_train in [*test['PAIS'].unique()]: # De los valos que hay en test\n",
    "    if val_train not in [*train['PAIS'].unique()]: #Cuales no están en train?\n",
    "        pais_lista.append(val_train)\n",
    "    else:\n",
    "        pass\n",
    "print(len(pais_lista))\n",
    "print(data[data['PAIS'].isin(pais_lista)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LISTO(dropeado más abajo)\n",
    "nro_documento_lista = []\n",
    "for val_train in [*test['NUMERO DE DOCUMENTO'].unique()]: # De los valos que hay en test\n",
    "    if val_train not in [*train['NUMERO DE DOCUMENTO'].unique()]: #Cuales no están en train?\n",
    "        nro_documento_lista.append(val_train)\n",
    "    else:\n",
    "        pass\n",
    "print(len(nro_documento_lista))\n",
    "print(data[data['NUMERO DE DOCUMENTO'].isin(nro_documento_lista)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LISTO(dropeado más abajo)\n",
    "nombre_razon_social = []\n",
    "for val_train in [*test['NOMBRE/RAZON SOCIAL'].unique()]: # De los valos que hay en test\n",
    "    if val_train not in [*train['NOMBRE/RAZON SOCIAL'].unique()]: #Cuales no están en train?\n",
    "        nombre_razon_social.append(val_train)\n",
    "    else:\n",
    "        pass\n",
    "print(len(nombre_razon_social))\n",
    "print(data[data['NOMBRE/RAZON SOCIAL'].isin(nombre_razon_social)].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Si bien hay formas de corregir estos casos, al final, eliminaremos las filas con fines prácticos... ¿Por qué? Osea, no tenemos forma de saber cuánto se exportará de tal subpartida si es que no tenemos data pasada de ella... Por eso lo eliminaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos primero este caso, porque es el que contienen más filas...\n",
    "to_drop = data[data['NUMERO DE DOCUMENTO'].isin(nro_documento_lista)].index\n",
    "data.drop(to_drop,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aja!!! Tenemos dos casos... En Malawi y Macedonia... Donde el número de documento es -1 (casos a eliminar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora evaluaremos pero en otra columna (Es logico que en Nombre/razon social no haya ninguno más)\n",
    "#Dropeando PAIS\n",
    "to_drop = data[data['PAIS'].isin(pais_lista)].index\n",
    "data.drop(to_drop, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuántos números de documento con -1 existen?  -> 182\n",
    "\n",
    "- En un caso ideal, se debería hacer una dummy y preguntarle al negocio **¿Por qué en estos casos los números de documento son -1?** Si es una respuesta específica se debe **transmitir eso al modelo...** También, se debería de ver si los valores obedecen a un patrón específico (Por ejemplo, todos los que exportan menos de 100kg pueden tener nro de doc -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropeando nro de doc == -1\n",
    "to_drop = data[data['NUMERO DE DOCUMENTO'] == -1].index\n",
    "data.drop(to_drop,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropeando subpartida\n",
    "to_drop = data[data['SUBPARTIDA'].isin(subpartida_lista)].index\n",
    "data.drop(to_drop,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pequeño feature-engineering para conocer más de la empresa:\n",
    "\n",
    "Recuerda que la empresa no solamente puede exportar una vez, sino puede hacerlo varias veces en un mes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = pd.crosstab(data['NOMBRE/RAZON SOCIAL'],data['MES'])\n",
    "cross['suma_fq_exp_en_meses_total'] = cross.iloc[:,:6].sum(axis=1)\n",
    "cross['suma_fq_exp_para_x_train'] = cross.iloc[:,:4].sum(axis=1)\n",
    "cross['suma_fq_exp_para_x_test'] = cross.iloc[:,1:5].sum(axis=1)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "cross.columns = ['fq_exp_enero','fq_exp_feb','fq_exp_marz','fq_exp_abr','fq_exp_may','fq_exp_jun',\n",
    "                 'suma_fq_exp_en_meses_total','suma_fq_exp_para_x_train','suma_fq_exp_para_x_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No tendremos de otra para predecir 0 en estos casos. Es sabiduría del negocio saber la periodicidad de sus clientes. \n",
    "#Como no poseeemos esa información, no tendremos más que ayudar a nuestro modelo a predecir esos casos que serán 0\n",
    "cross['dummy_ayuda_train'] = 0\n",
    "cross.loc[(cross['suma_fq_exp_para_x_train'] == 0) & (cross['fq_exp_may'] > 0),'dummy_ayuda_train'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross['dummy_ayuda_test'] = 0\n",
    "cross.loc[(cross['suma_fq_exp_para_x_test'] == 0) & (cross['fq_exp_jun']>0),'dummy_ayuda_test'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross tiene las frecuencias de registros que hay para cada empresa..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(asdasdjasodj1ojo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Voy a resumir mis periodos en tablas, para luego hacerles merge con esta funcion: ?? DEBERIA?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Esta función va a unir cada dataset que tengo dentro de una lista y lo va a poner como lags...\n",
    "def merge_my_lags(master_cons,lista_df, starts_with = 1):\n",
    "  for i,df in enumerate(lista_df,starts_with):\n",
    "     #Saco numero de campana\n",
    "    cols_format = ['L{}_'.format(i)+col_name for col_name in df.columns] #Le cambio los nombres a las cols\n",
    "    cols_format[0] = 'idconsultora'\n",
    "    cols_format.remove('L{}_'.format(i)+'campana')\n",
    "    df_temp = df.drop('campana',axis=1)\n",
    "    df_temp.columns = cols_format\n",
    "    master_cons = master_cons.merge(df_temp,how='outer',on='idconsultora',suffixes=('',''))\n",
    "  return master_cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero tengo que dividir mis x_train y x_test\n",
    "months_x_train = [1,2,3,4]\n",
    "months_x_test = [2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haré un diccionario de cómo quiero resumir estas variables...\n",
    "ft = {'MES':'nunique', 'FOB (US$)':['max','min','mean','median','var','std','nunique','size','sum'],\n",
    "     'PESO NETO (KG)':['max','min','median','var','std','mean'],'SECTOR SUNAT':['nunique','size'],\n",
    "     'SUBPARTIDA':['nunique','size'], 'US$/kg':['max','min','median','var','std','mean']}\n",
    "\n",
    "data_agrupada_mes_emp = data.groupby(['NOMBRE/RAZON SOCIAL','MES']).agg(ft).reset_index()\n",
    "#Pondremos de ejemplo a la empresa ZOWI PERU SOCIEDAD ANONIMA CERRADA... \n",
    "data_agrupada_mes_emp.columns = ['%s%s' % (a, '_%s' % b if b else '') for a,b in data_agrupada_mes_emp.columns]\n",
    "data_agrupada_mes_emp[data_agrupada_mes_emp['NOMBRE/RAZON SOCIAL'] == 'ZOWI PERU SOCIEDAD ANONIMA CERRADA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "#Transformaré de manera conveniente la tabla que habíamos creado \n",
    "pivot_table = pd.pivot_table(data_agrupada_mes_emp, values=[*data_agrupada_mes_emp.iloc[2:].columns.values],\n",
    "              index=['NOMBRE/RAZON SOCIAL'], columns = ['MES'])\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pivot_table.columns = ['%s%s' % (b, '_%s' % a if a else '') for a,b in pivot_table.columns]\n",
    "pivot_table = pivot_table.reset_index()\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, llenaremos los nulos con 0 porque se entiende de que son cero al no tener data o solamente 1 valor... **VAMOS A VER QUË TAL LE VA AL ALGORITMO** \n",
    "\n",
    "- NOTA: **SE DEBERIA AGREGAR AUTO-CORRELACIONES CON LOS PERIODOS PASADOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in pivot_table.columns:\n",
    "    pivot_table[col].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a hacer el train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_train = [col for col in pivot_table.columns if col.startswith(('1','2','3','4'))]\n",
    "cols_train.append('NOMBRE/RAZON SOCIAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_test = [col for col in pivot_table.columns if col.startswith(('2','3','4','5'))]\n",
    "cols_test.append('NOMBRE/RAZON SOCIAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = y_train - y_test\n",
    "diff.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pivot_table[cols_train]\n",
    "y_train = pivot_table['5_US$/kg_mean']\n",
    "cols_train.append('5_US$/kg_mean')\n",
    "x_train = pivot_table[cols_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pivot_table[cols_test]\n",
    "y_test = pivot_table['6_US$/kg_mean']\n",
    "cols_test.append('6_US$/kg_mean')\n",
    "x_test = pivot_table[cols_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train - X_test['5_US$/kg_mean']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero aún falta:\n",
    "\n",
    "- Agregar la data de cross.\n",
    "- Estandarizar el nombre de las columnas para x_train y x_test. Así como para el target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = cross.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_train = cross[['NOMBRE/RAZON SOCIAL','fq_exp_enero','fq_exp_feb','fq_exp_marz','fq_exp_abr','dummy_ayuda_train',\n",
    "      'suma_fq_exp_para_x_train']]\n",
    "cross_test = cross[['NOMBRE/RAZON SOCIAL','fq_exp_feb','fq_exp_marz','fq_exp_abr','fq_exp_may','dummy_ayuda_test',\n",
    "      'suma_fq_exp_para_x_test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.join(cross_train.set_index('NOMBRE/RAZON SOCIAL'), on='NOMBRE/RAZON SOCIAL')\n",
    "X_test = X_test.join(cross_test.set_index('NOMBRE/RAZON SOCIAL'), on='NOMBRE/RAZON SOCIAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.columns)\n",
    "print(X_test.columns)\n",
    "\n",
    "#Voy a estandarizar mis columnas entonces\n",
    "\n",
    "X_test.columns = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train - X_test['4_US$/kg_mean']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a hacerle feature encoding a la columna del nombre de las empresas... \n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None, \n",
    "                  tst_series=None, \n",
    "                  target=None, \n",
    "                  min_samples_leaf=1, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior  \n",
    "    \"\"\" \n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean \n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index \n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_trn, temp_sub = target_encode(X_train['NOMBRE/RAZON SOCIAL'],\n",
    "                                  X_test['NOMBRE/RAZON SOCIAL'],\n",
    "                                  target = y_train,\n",
    "                                  min_samples_leaf = 15,\n",
    "                                  smoothing = 2,\n",
    "                                  noise_level = 0.01)\n",
    "X_train['NOMBRE/RAZON SOCIAL'] = temp_trn\n",
    "X_test['NOMBRE/RAZON SOCIAL'] = temp_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veremos a los outliers con el coeficiente de Kurtosis... trataremos de reducirlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',112) #Abro para desplegar todo\n",
    "X_train.kurtosis().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.kurtosis().sort_values()\n",
    "pd.set_option('display.max_rows',40) #Cierro el despliegue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop_test = [*y_test[y_test > 20000].index]\n",
    "to_drop_train = [*y_train[y_train > 20000].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in to_drop_train:\n",
    "    to_drop_test.append(elem)\n",
    "to_drop_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(to_drop_test,axis=0,inplace=True)\n",
    "X_test.drop(to_drop_test, axis=0,inplace=True)\n",
    "y_train.drop(to_drop_test,inplace=True)\n",
    "y_test.drop(to_drop_test,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train - y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a sacarle logaritmos a todos\n",
    "cols_to_log = [col for col in X_train.columns if col.startswith(('1','2','3','4','5'))]\n",
    "cols_to_log = [col for col in cols_to_log if not 'unique' in col and 'size' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_to_log:\n",
    "    X_train[col] = np.log1p(X_train[col])\n",
    "    X_test[col] = np.log1p(X_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lo mejor tendremos que encodear denuevo (baja la kurtosis)\n",
    "temp_trn, temp_sub = target_encode(X_train['NOMBRE/RAZON SOCIAL'],\n",
    "                                  X_test['NOMBRE/RAZON SOCIAL'],\n",
    "                                  target = y_train,\n",
    "                                  min_samples_leaf = 15,\n",
    "                                  smoothing = 2,\n",
    "                                  noise_level = 0.01)\n",
    "X_train['NOMBRE/RAZON SOCIAL'] = temp_trn\n",
    "X_test['NOMBRE/RAZON SOCIAL'] = temp_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viendo autocorrelaciones y botando multicolinealidad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo vermeos en la data continua (que sera la que está dentro de cols_to_log)\n",
    "pd.set_option('display.max_rows',None)\n",
    "X_train[cols_to_log].corr().abs().unstack().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',40) #Cierro el despliegue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a dropear la mediana y la varianza de nuestro modelo? ... Haremos que eso lo decida nuestro primer modelo... \n",
    "# Por ahora solo dropearemos la std\n",
    "\n",
    "cols_final = [col for col in cols_to_log if not 'std' in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionando nuestro modelo:\n",
    "\n",
    "- Haremos Cross Validation. \n",
    "- Probaremos solo modelos basado en árbol (los de regresión parecen no funcionar con esta cantidad de data)\n",
    "- Eligiremos nuestro(s) modelos... \n",
    "- Tunearemos nuestros hiperparámetros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for fold in range(2,13):\n",
    "    tree = DecisionTreeRegressor(max_depth = fold, random_state=123)\n",
    "    s = cross_val_score(tree, X_train, y_train, cv=10, scoring = \"neg_mean_absolute_error\")\n",
    "    print('------------------------Este será nuestro score para {} max_depth'.format(fold))\n",
    "    print(s)\n",
    "    print('La media será de {}'.format(s.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = X_train.join(y_train, how='outer')\n",
    "X_2 = X_test.join(y_test,how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1['5_US$/kg_mean'] = X_1['5_US$/kg_mean'].rename('target')\n",
    "X_2['6_US$/kg_mean'] = X_2['6_US$/kg_mean'].rename('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1.to_csv('nuevo_train.csv',index=False)\n",
    "X_2.to_csv('nuevo_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voy a tunear los hiperparámetros en la nube.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_ind = ['1_SECTOR SUNAT_nunique',\n",
    " '2_SECTOR SUNAT_nunique',\n",
    " '3_SECTOR SUNAT_nunique',\n",
    " '4_SECTOR SUNAT_nunique',\n",
    " '1_SECTOR SUNAT_size',\n",
    " '2_SECTOR SUNAT_size',\n",
    " '3_SECTOR SUNAT_size',\n",
    " '4_SECTOR SUNAT_size','fq_exp_enero',\n",
    " 'fq_exp_feb',\n",
    " 'fq_exp_marz',\n",
    " 'fq_exp_abr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_index = [X_train.columns.get_loc(col) for col in cat_ind if col in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "params_lgb = {\n",
    "    'boosting_type':'dart',\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators':68,\n",
    "    'max_depth': -1\n",
    "}\n",
    "lgb_reg = LGBMRegressor(**params_lgb, n_jobs=-1, random_state=123)\n",
    "lgb_reg.fit(X_train, y_train, eval_set=(X_test,y_test), eval_metric='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sin cat_ind:\n",
    "preds = lgb_reg.predict(X_test)\n",
    "mean_absolute_error(y_test, preds)\n",
    "\n",
    "# mean_absolute_error(y_test, np.where(preds > 0, preds, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diferencia = y_test - preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(diferencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.where(preds > 0, preds, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, np.where(preds > 0, preds, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acá irá la importancia de variables... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Quienes son los outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross.iloc[to_drop_test]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
