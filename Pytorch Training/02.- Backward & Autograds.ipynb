{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograds\n",
    "\n",
    "Cada tensor tiene un parámetro booleano ```requires_grad``` que nos permitirá excluir o incluir la construcción de gráficos de dependencia para hacer debugging de las gradientes de nuestro algoritmos.  Si alguno de los tensores necesita ser diferenciado, entonces tendremos que activar este parámetro antes. El poder activar y desactivar gradientes en ciertos tensores nos permite \"congelar\" ciertos layers en las redes neuronales, para poder re-entrenar un modelo (finetuning).\n",
    "\n",
    "Tal como se mencionó antes, nos sirve para crear gráficos de dependecia pero ¿Por qué esto es necesario? Cuando hacemos la optimización de nuestro algoritmo descenso de la gradiente, y **tener un registro de las gradientes** nos permite hacer esto, es decir, hacer un seguimiento de los errores a través de la red.\n",
    "\n",
    "Entonces, recordemos que en una red neuronal pasan dos cosas: \n",
    "\n",
    "- Forward Propagation: La red neuronal hace su mejor predicción y pasa todos los datos del input a través de las neuronas y capas que contenga, con el fin de hacer su mejor predicción.\n",
    "\n",
    "- Backward Propagation: La red neuronal propaga el error a través de las derivadas y debe de tener una colección de ellas (date cuenta en este caso es necesita el ``requires_grad`` activo) optimizando los parámetros usando el descenso de la gradiente. En este sentido, torch.autograd es una herramienta para computar vectorialmente una matriz producto Jacobiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mlp_mnist_model import MLP #Es necesario tener la estructura del algoritmo en un archivo aparte.\n",
    "\n",
    "#Cargamos nuestro modelo\n",
    "model = torch.load('mnist_model.pt')\n",
    "\n",
    "#Cargamos datos generados de manera aleatoria (Solo para actualizar los parámetros)\n",
    "data = torch.rand(64, 784) #Tendré 64 muestras de un tensor de 784 (lo q acepta mi red)\n",
    "labels = torch.rand(64, 10) #Los labels con probabilidades "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predecimos para ver qué tanto nuestro algoritmo clasifica el ruido\n",
    "output = model(data)\n",
    "pred = torch.argmax(output, dim=1)\n",
    "\n",
    "#Vamos a definir la pérdida (loss) de la sgte manera\n",
    "loss = (output - labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos un optimizador\n",
    "optim = torch.optim.SGD(model.parameters(), lr= 1e-2, momentum= 0.9)\n",
    "#Inicializamos el descenso de la gradiente\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
