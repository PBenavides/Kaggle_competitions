{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sesion6_NLP_Clasificacion_Analisis_Sentimiento.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf8amewP3xeb",
        "colab_type": "text"
      },
      "source": [
        "# **Presentación del caso: Análisis de Sentimientos con Twitter**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqZ4QkZXecXk",
        "colab_type": "text"
      },
      "source": [
        "Este caso muestra un conjunto de **13,962 tweets** en la data de entrenamiento. Cuenta con una la columna *label* (etiqueta u objetivo) donde se registra **dos valores**:\n",
        "1.   0: tweet positivo sin mención de temas políticos, racistas, etc\n",
        "2.   1: tweet negativo que menciona temas políticos, racistas, etc\n",
        "![Image of Yaktocat](https://s3.amazonaws.com/thinkific/file_uploads/118220/images/0c8/229/64d/1549269497113.jpg)\n",
        "\n",
        "Para tratar la información contenida en los tweets, **data no estructurada**, procederemos a utilizar la librería **nltk** -Natural Language Tokenization- y los siguientes modelos de NLP -Natural Language Processing para procesar los textos:\n",
        "1.   BOW: Bag of Words\n",
        "2.   TF-IDF\n",
        "3.   W2V: Word to Vec\n",
        "\n",
        "Finalmente para realizar las predicciones complementamos a los modelos de NLP antes mencionados varios modelos de clasificación para predecir la columna label por tratarse de dos categorías (0 y 1).\n",
        "\n",
        "\n",
        "Los detalles de la competencia **gratuita** pueden encontrarse en la siguiente [web](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOSdmtwCHokE",
        "colab_type": "text"
      },
      "source": [
        "# **Comprensión de los datos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgLxZwwc598f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CASO N°1\"\n",
        "#Importamos las librerías necesarias \n",
        "import pandas as pd\n",
        "import re    # regular expression nos permite editar la visualización de los caracteres según nuestras prefencias\n",
        "import nltk  # natural language tokenization es una librería para manipular las palabras que encontraremos en los tweets\n",
        "import string # exclusivamente para palabras \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "pd.set_option(\"display.max_colwidth\", 200) #Definimos un máximo de ancho de columna por la naturaleza del largo de los textos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o_VJey44TK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°2\"\n",
        "#Leemos los datos de entrenamiento -train- y los datos de testing -test\n",
        "train=pd.read_csv('https://raw.githubusercontent.com/javalpe/datasets/master/train_twitter_analysis.csv', index_col=\"id\")\n",
        "summit=pd.read_csv('https://raw.githubusercontent.com/javalpe/datasets/master/test_twitter_analysis.csv', index_col=\"id\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3wDEKCMEJYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°3\"\n",
        "#Visualizamos el tamaño de cada dataset con el comando shape\n",
        "train.shape, summit.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkDrRcB57-JZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°4\"\n",
        "#Visualizamos el contenido de los tweets con el label = 0 que serían considerados positivos\n",
        "train[train['label'] == 0].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj5eXXYm83SH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°5\"\n",
        "#Visualizamos el contenido de los tweets con el label = 1 que serían considerados negativos\n",
        "train[train['label'] == 1].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvBqSPsy9Iqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°6\"\n",
        "#Podemos distinguir la cantidad de tweets positivos y negativos con el método value_counts()\n",
        "train[\"label\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjLUIj2Q_k6l",
        "colab_type": "text"
      },
      "source": [
        "Como la cantidad de tweets positivos es **15 veces mayor** de los tweets negativos podemos concluir que tenemos **data desbalanceada**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4_21L6w_j97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°7\"\n",
        "#Visualizamos el largo de los tweets -número de caracteres- en train y test con el método str.len()\n",
        "length_train = train['tweet'].str.len() \n",
        "length_summit = summit['tweet'].str.len() \n",
        "plt.hist(length_train, bins=20, label=\"train_tweets\") #el método hist de la librería matplotlib (plt) permite dibujar histogramas\n",
        "plt.hist(length_summit, bins=20, label=\"summit_tweets\") #el parámetro label es para identificar estos datos para una leyenda del gráfico\n",
        "plt.legend() #el comando legend permite añadir una leyenda a nuestra gráfica\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-flHGjBOw5m",
        "colab_type": "text"
      },
      "source": [
        "Adicionalmente se visualiza que la cantidad de caracteres de los tweets positivos es mucho mayor que los tweets negativos. Por ello podemos concluir que el modelo final será **más preciso para predecir tweets positivos que negativos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7XWodKNBXoF",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocesamiento de los datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFUJZQGqBkqT",
        "colab_type": "text"
      },
      "source": [
        "## **Extracción de los caracteres especiales**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFN0PppRAmbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°8\"\n",
        "#Inicialmente juntamos train y test para realizar este tratamiento para ambos dataset. Luego volveremos a dividirlos\n",
        "combi = train.append(summit, ignore_index=True) \n",
        "combi.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3JWRtA1B_UN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°9\"\n",
        "#Definimos una función para eliminar de los tweets alguna palabra, simbolo o caracter especial (que irá en el parámetro pattern) que creamos conveniente\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "    return input_txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abl-jc8ODmfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°10\"\n",
        "#Quitamos todas aquellas menciones dentro de los tweets a otros usuarios (que comienzan con @)\n",
        "combi['tweet_limpio'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")\n",
        "combi.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0IxTDBHFqPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°11\"\n",
        "#Reemplazamos todos aquellos carácteres especiales como signos de puntuación por un espacio en blanco\n",
        "combi['tweet_limpio'] = combi['tweet_limpio'].str.replace(\"[^a-zA-Z#]\", \" \") #el simbolo ^ signfica \"todo menos\" en este caso palabras y números \n",
        "combi.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB255XNiGf6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°12\"\n",
        "#Removemos todas aquellas palabras con un largo de 3 caracteres o menos que podrían incluir conectores, advervios, artículos (por ejemplo a, and, the)\n",
        "combi['tweet_limpio'] = combi['tweet_limpio'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "combi.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAsClMSCI0mM",
        "colab_type": "text"
      },
      "source": [
        "## **Tokenización y Steamming**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO7z6_TSI9Xf",
        "colab_type": "text"
      },
      "source": [
        "Para poder evaluar cada tweet será necesario dividirlo en palabras, este proceso se denomina *tokenization*.\n",
        "\n",
        "También realizaremos el proceso denominado *steamming* para solo utilizar las raíces de las palabras en el análisis (por ejemplo \"read\" en vez de \"reading\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6gPUqADHjR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°13\"\n",
        "#Dividimos los tweets en palabras y guardamos el resultado en una nueva columna denominada tweet_limpio\n",
        "tokenized_tweet = combi['tweet_limpio'].apply(lambda x: x.split()) #el comando split signfica dividir\n",
        "tokenized_tweet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9jgL_TfKA3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°14\"\n",
        "#Ahora procederemos a extraer solo la raíz de las palabras con el método stem.porter y actualizamos la columna tweet_limpio\n",
        "from nltk.stem.porter import * \n",
        "stemmer = PorterStemmer() \n",
        "stemmed_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0ZXUEwqKiwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°15\"\n",
        "#Finalmente juntamos cada raíz de palabra para almacenarla nuevamente en la columna tweet_limpio\n",
        "for i in range(len(stemmed_tweet)):\n",
        "    stemmed_tweet[i] = ' '.join(stemmed_tweet[i])    \n",
        "combi['tweet_limpio'] = stemmed_tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVx12ijWL8iL",
        "colab_type": "text"
      },
      "source": [
        "## **Aplicaciones de visualización**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV7doBonMDx_",
        "colab_type": "text"
      },
      "source": [
        "### 1. *WORDCLOUD*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNQhuw2HUBuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°16\"\n",
        "#Importamos la librería WordCloud para visualizar las palabras más usadas (que aparecen en mayor tamaño)\n",
        "from wordcloud import WordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To6GdGrDOFkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°17\"\n",
        "#Mostramos las palabras más usadas\n",
        "all_words = ' '.join([text for text in combi['tweet_limpio']])\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) \n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcf4GrYKNaNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°18\"\n",
        "#Mostramos las palabras más usadas en los tweets positivos\n",
        "normal_words =' '.join([text for text in combi['tweet_limpio'][combi['label'] == 0]]) \n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aRZs-RgOmYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°19\"\n",
        "#Mostramos las palabras más usadas en los tweets negativos\n",
        "negative_words =' '.join([text for text in combi['tweet_limpio'][combi['label'] == 1]]) \n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K44SVviSUd20",
        "colab_type": "text"
      },
      "source": [
        "### 2. *HASHTAG MÁS USADOS*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg34kkGKO9K9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°20\"\n",
        "# Definimos una función que nos permita extraer los hashtags (#) de los tweets\n",
        "def hashtag_extract(x):\n",
        "    hashtags = []\n",
        "    for i in x:\n",
        "        ht = re.findall(r\"#(\\w+)\", i)\n",
        "        hashtags.append(ht)\n",
        "    return hashtags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPZPRWdEovQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°21\"\n",
        "#Extraemos los hashtags (#) de los tweets que no son racistas ni sexistas (label=0) y guardamos el resultado en la variable HT_positivos\n",
        "HT_positivos = hashtag_extract(combi['tweet_limpio'][combi['label'] == 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKsMwcnWozED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°22\"\n",
        "#Extraemos los hashtags (#) de los tweets que sí son racistas ni sexistas (label=1) y guardamos el resultado en la variable HT_negativos \n",
        "HT_negativos = hashtag_extract(combi['tweet_limpio'][combi['label'] == 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETr9667Co1d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°23\"\n",
        "#Para poder realizar un conteo y graficarlo en barras procedemos a unir todas las palabras de cada variable\n",
        "HT_positivos = sum(HT_positivos,[])\n",
        "HT_negativos = sum(HT_negativos,[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMYvLjxjqI2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°24\"\n",
        "#Con ayuda del método FreqDist realizamos el conteo y graficamos las frecuencias de aparición de cada hashtag en los tweets de la variable HT_regular (label=0)\n",
        "a = nltk.FreqDist(HT_positivos)\n",
        "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
        "                  'Count': list(a.values())})\n",
        "d = d.nlargest(columns=\"Count\", n = 10) #Para efectos de visualización solo nos quedaremos con el top10\n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\") #Para\n",
        "ax.set(ylabel = 'Count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtX9ZK2gs5ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°25\"\n",
        "#Con ayuda del método FreqDist realizamos el conteo y graficamos las frecuencias de aparición de cada hashtag en los tweets de la variable HT_negative (label=1)\n",
        "b = nltk.FreqDist(HT_negativos)\n",
        "e = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n",
        "e = e.nlargest(columns=\"Count\", n = 10)   #Para efectos de visualización solo nos quedaremos con el top10\n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\n",
        "ax.set(ylabel = 'Count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T5wMKCmaMWw",
        "colab_type": "text"
      },
      "source": [
        "## **Bag of Words (BOW)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2lKhxkEJ1Lk",
        "colab_type": "text"
      },
      "source": [
        "El primer método para preparar la data y poder ejecutar modelos de clasificación se denomina **Bag of Words** donde se obtiene un dataset compuesto por D documentos (que representan filas o registros) y N corpus (que representan las variables predictoras)\n",
        "\n",
        "Para nuestro ejemplo el objetivo es obtener como variables predictoras (columnas o *features*) las palabras y las filas o registros serán cada tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGjHab30Jsc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°26\"\n",
        "#Importamos la librería CountVectorizer de sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bow_vectorizer = CountVectorizer(analyzer='word',stop_words='english') #en nuestro caso estamos procesando tweets en inglés\n",
        "bow = bow_vectorizer.fit_transform(combi['tweet_limpio']) #guardamos el resultado en una variable denominada bow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zztKR0Z9Lq6c"
      },
      "source": [
        "## **TF - IDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb0_wIoULqZR",
        "colab_type": "text"
      },
      "source": [
        "El segundo método para preparar la data y poder ejecutar modelos de clasificación se denomina **TF - IDF** donde se obtiene un dataset conformado por los mismos elementos. Sin embargo vamos a añadir dos elementos de evaluación (como pesos para cada variable):\n",
        "\n",
        "\n",
        "1.   TF = # veces que aparece una palabra en un solo documento / # palabras totales de ese mismo documento\n",
        "2.   IDF = log (N/n) n es el # veces que aparece una palabra en cada documento y N es el # total de documentos\n",
        "\n",
        "\n",
        "Para nuestro ejemplo recordemos que un documento es un tweet y una palabra es cada una de las palabras que hemos tokenizado y steammeado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If_A9dU2N-Q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°27\"\n",
        "#Importamos la librería TfidVectorizer de sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(analyzer='word',stop_words='english') #en nuestro caso estamos procesando tweets en inglés\n",
        "tfidf = tfidf_vectorizer.fit_transform(combi['tweet_limpio']) #guardamos el resultado en una variable denominada tfidf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hKYNs5D0Q0AS"
      },
      "source": [
        "## **Word2Vec (w2v)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "62ZB0EbFQ4Q-"
      },
      "source": [
        "El tercer método para preparar la data y poder ejecutar modelos de clasificación se denomina **Word2Vec** donde cada palabra se transforma en un vector para relacionar las palabras entre sí antes de generar el dataset. Esto nos trae dos ventajas:\n",
        "\n",
        "\n",
        "1.   Se reduce significativamente la cantidad de columnas generadas\n",
        "2.   Se puede distinguir dos significados distintos para una misma palabra (por ejemplo *apple* puede significar fruta y también signficar empresa tecnológica)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnK_Hmo-OfF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°28\"\n",
        "#Dividimos nuevamente la columna tweet_limpio palabra por palabra para poder realizar el análisis y guardamos el resultado en la variable tokenized_tweet\n",
        "tokenized_tweet = combi['tweet_limpio'].apply(lambda x: x.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWtEOV74UPDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°29\"\n",
        "#Importamos las librerías que necesitamos para ejecutar el análisis Word2vec\n",
        "import gensim\n",
        "from gensim.models.doc2vec import LabeledSentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhIvVLSZT1ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°30\"\n",
        "#Generamos un modelo word2vec y guardamos el modelo en la variable model_w2v\n",
        "model_w2v = gensim.models.Word2Vec(tokenized_tweet,\n",
        "                                   window=5,\n",
        "                                   size= 300,\n",
        "                                   min_count=2,\n",
        "                                   alpha=0.03,\n",
        "                                   min_alpha=0.0007,\n",
        "                                   negative=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTZ5CAEBZz2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°31\"\n",
        "#Entrenamos el modelo con los tweets tokenizados\n",
        "model_w2v.train(tokenized_tweet, total_examples= len(combi['tweet_limpio']), epochs=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tTtBYInjWMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°32\"\n",
        "#Definimos una función para generar un vector para cada uno de los tweet\n",
        "def word_vector(tokens, size):\n",
        "    vec = np.zeros(size).reshape((1, size))\n",
        "    count = 0.\n",
        "    for word in tokens:\n",
        "        try:\n",
        "            vec += model_w2v[word].reshape((1, size))\n",
        "            count += 1.\n",
        "        except KeyError: continue\n",
        "        if count != 0: vec/= count\n",
        "    return vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opRbFRDxmk6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°33\"\n",
        "#Aplicamos la función sobre cada tweet y generamos un dataframe que guardaremos como la variable wordvec_df\n",
        "wordvec_arrays = np.zeros((len(tokenized_tweet), 300)) \n",
        "for i in range(len(tokenized_tweet)):\n",
        "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 300)\n",
        "    wordvec_df = pd.DataFrame(wordvec_arrays)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8Ij_n_Jm-ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°34\"\n",
        "#Comprobamos el tamaño de cada variable creada\n",
        "bow.shape, tfidf.shape, wordvec_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KayjIMIKe-vI",
        "colab_type": "text"
      },
      "source": [
        "# **Modelamiento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2myCykiqEAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°35\"\n",
        "#Dividimos la variable bow en train y summit originales\n",
        "train_bow = bow[:31962,:] #Esta parte de la data sí contiene el label\n",
        "summit_bow = bow[31962:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwCWCDcxqOAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°36\"\n",
        "#Dividimos la variable tfidf en train y summit originales\n",
        "train_tfidf = tfidf[:31962,:] #Esta parte de la data sí contiene el label\n",
        "summit_tfidf = tfidf[31962:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkawJR55qWT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°37\"\n",
        "#Dividimos la variable wordvec_df en train y summit originales\n",
        "train_w2v = wordvec_df.iloc[:31962,:] #Esta parte de la data sí contiene el label\n",
        "summit_w2v = wordvec_df.iloc[31962:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELhvEN22g_iS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°38\"\n",
        "#Utilizamos el método train_test_split para dividir train y test\n",
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain_bow, Xtest_bow, ytrain_bow, ytest_bow = train_test_split(train_bow, train['label'], test_size=0.15)\n",
        "Xtrain_tfidf, Xtest_tfidf, ytrain_tfidf, ytest_tfidf = train_test_split(train_tfidf, train['label'], test_size=0.15)\n",
        "Xtrain_w2v, Xtest_w2v, ytrain_w2v, ytest_w2v = train_test_split(train_w2v, train['label'], test_size=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzeyCes3n6RV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°39\"\n",
        "#Importamos las librerías útiles para todos los modelos\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWOF1_KyfGdx",
        "colab_type": "text"
      },
      "source": [
        "## **Regresión Logística**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbb7-8FVjNRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°40\"\n",
        "#Importamos las librerías de Regresión Logística y guardamos el modelo de Regresión Logística en la variable lreg\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lreg = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Fd9KHHKyh6",
        "colab_type": "text"
      },
      "source": [
        "### 1. *Regresión Logística a través de BOW*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIBP1QT8v6rJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°41\"\n",
        "#Entrenamos el modelo de Regresión Logística con Xtrain_bow, ytrain_bow y generamos una lista de accuracy score a partir de Xtest_bow, ytest_bow\n",
        "lreg.fit(Xtrain_bow, ytrain_bow)\n",
        "proba_rl_bow = lreg.predict_proba(Xtest_bow)[:,1]\n",
        "list_accuracy_bow=[]\n",
        "for punto_de_corte in range(0,100):\n",
        "    pred_rl_bow = [1 if x >= punto_de_corte/100 else 0 for x in proba_rl_bow]\n",
        "    list_accuracy_bow.append(accuracy_score(ytest_bow, pred_rl_bow))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDYZENNPwiiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°42\"\n",
        "#Dibujamos los puntos de corte con sus respectivos score\n",
        "xs = [x/100 for x in range(0,100)]\n",
        "ys = list_accuracy_bow\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.grid(True)\n",
        "plt.xlabel('Punto de corte')\n",
        "plt.ylabel('list_accuracy_bow')\n",
        "plt.plot(xs, ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7EUCSz-KxCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°43\"\n",
        "#Actualizamos las predicciones de BOW con el punto de corte óptimo\n",
        "pred_rl_bow = proba_rl_bow >= 0.19\n",
        "pred_rl_bow = pred_rl_bow.astype(np.int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as6Hb_2fLEqY",
        "colab_type": "text"
      },
      "source": [
        "### 2. *Regresión Logística a través de TF-IDF*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oY-z7plLy2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°44\"\n",
        "#Entrenamos el modelo de Regresión Logística con Xtrain_tfidf, ytrain_tfidf y generamos una lista de accuracy score a partir de Xtest_tfidf, ytest_tfidf\n",
        "lreg.fit(Xtrain_tfidf, ytrain_tfidf)\n",
        "proba_rl_tfidf = lreg.predict_proba(Xtest_tfidf)[:,1]\n",
        "list_accuracy_tfidf=[]\n",
        "for punto_de_corte in range(0,100):\n",
        "    pred_rl_tfidf = [1 if x >= punto_de_corte/100 else 0 for x in proba_rl_tfidf]\n",
        "    list_accuracy_tfidf.append(accuracy_score(ytest_tfidf, pred_rl_tfidf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd0o9-Fwz6Eu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°45\"\n",
        "#Dibujamos los puntos de corte con sus respectivos score\n",
        "xs = [x/100 for x in range(0,100)]\n",
        "ys = list_accuracy_tfidf\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.grid(True)\n",
        "plt.xlabel('Punto de corte')\n",
        "plt.ylabel('list_accuracy_tfidf')\n",
        "plt.plot(xs, ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx_vkRfq0MXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°46\"\n",
        "#Actualizamos las predicciones de TF-IDF con el punto de corte óptimo\n",
        "pred_rl_tfidf = proba_rl_tfidf >= 0.17\n",
        "pred_rl_tfidf = pred_rl_tfidf.astype(np.int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LUooulQUQkQv"
      },
      "source": [
        "### 3. *Regresión Logística a través de W2V*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iyctXiPwSQXL",
        "colab": {}
      },
      "source": [
        "\"CELDA N°47\"\n",
        "#Entrenamos el modelo de Regresión Logística con Xtrain_w2v, ytrain_w2v y generamos una lista de accuracy score a partir de Xtest_w2v, ytest_w2v\n",
        "lreg.fit(Xtrain_w2v, ytrain_w2v)\n",
        "proba_rl_w2v = lreg.predict_proba(Xtest_w2v)[:,1]\n",
        "list_accuracy_w2v=[]\n",
        "for punto_de_corte in range(0,100):\n",
        "    pred_rl_w2v = [1 if x >= punto_de_corte/100 else 0 for x in proba_rl_w2v]\n",
        "    list_accuracy_w2v.append(accuracy_score(ytest_w2v, pred_rl_w2v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHomLCFEdLDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°48\"\n",
        "#Dibujamos los puntos de corte con sus respectivos score\n",
        "xs = [x/100 for x in range(0,100)]\n",
        "ys = list_accuracy_w2v\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.grid(True)\n",
        "plt.xlabel('Punto de corte')\n",
        "plt.ylabel('list_accuracy_w2v')\n",
        "plt.plot(xs, ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBc-wJI8fvTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°49\"\n",
        "#Actualizamos las predicciones de W2V con el punto de corte óptimo\n",
        "pred_rl_w2v = proba_rl_w2v >= 0.18\n",
        "pred_rl_w2v = pred_rl_w2v.astype(np.int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUSy7Z5GXYqK",
        "colab_type": "text"
      },
      "source": [
        "## **RandomForest**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YakAW14fdQyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°50\"\n",
        "#Importamos la librería de RandomForestClassifier y guardamos el modelo de Random Forest en la variable rf\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf=RandomForestClassifier(max_depth=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGIvcc_4kj2P",
        "colab_type": "text"
      },
      "source": [
        "### 1. *Random Forest a través de BOW*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66xtuAQClGzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°51\"\n",
        "#Aplicamos el modelo de Random Forest en Xtrain_bow, ytrain_bow y generamos las predicciones a partir de Xtest_bow\n",
        "rf_bow = rf.fit(Xtrain_bow, ytrain_bow)\n",
        "pred_rf_bow = rf_bow.predict(Xtest_bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQp2nOTZkqN0",
        "colab_type": "text"
      },
      "source": [
        "### 2. *Random Forest a través de TF-IDF*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GggwVCHoXk8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°52\"\n",
        "#Aplicamos el modelo de Random Forest en Xtrain_tfidf, ytrain_tfidf y generamos las predicciones a partir de Xtest_tfidf\n",
        "rf_tfidf = rf.fit(Xtrain_tfidf, ytrain_tfidf)\n",
        "pred_rf_tfidf = rf_tfidf.predict(Xtest_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACcOn_ykk62A",
        "colab_type": "text"
      },
      "source": [
        "### 3. *Random Forest a través de W2V*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TumbQszYYaM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°53\"\n",
        "#Aplicamos el modelo de Random Forest en Xtrain_w2v, ytrain_w2v y generamos las predicciones a partir de Xtest_w2v\n",
        "rf_w2v = rf.fit(Xtrain_w2v, ytrain_w2v)\n",
        "pred_rf_w2v = rf_w2v.predict(Xtest_w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tSQC7AXFeJd6"
      },
      "source": [
        "## **XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9Q5LOFpeMOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°54\"\n",
        "#Importamos la librería de XGBClassifier y guardamos el modelo de XGBoost en la variable xgb\n",
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(learning_rate = 0.1,\n",
        "                      early_stopping_rounds = 10,\n",
        "                      min_child_weight=2,\n",
        "                      gamma=0.1,\n",
        "                      subsample=0.85,\n",
        "                      colsample_bytree=0.8,\n",
        "                      objective= 'binary:logistic') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehApw7RRndkK",
        "colab_type": "text"
      },
      "source": [
        "### 1. *XGBoost a través de BOW*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qP8C8UpeQv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°55\"\n",
        "#Aplicamos el modelo de XGBClassifier para los datos procesados según BOW \n",
        "xgb_bow = xgb.fit(Xtrain_bow, ytrain_bow) \n",
        "pred_xgb_bow = xgb_bow.predict(Xtest_bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8QRN_eono4C",
        "colab_type": "text"
      },
      "source": [
        "### 2. *XGBoost a través de TF-IDF*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EMEF40ftfXVK",
        "colab": {}
      },
      "source": [
        "\"CELDA N°56\"\n",
        "#Aplicamos el modelo de XGBClassifier para los datos procesados según TF-IDF \n",
        "xgb_tfidf = xgb.fit(Xtrain_tfidf, ytrain_tfidf) \n",
        "pred_xgb_tfidf = xgb_tfidf.predict(Xtest_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvU94O5inuYZ",
        "colab_type": "text"
      },
      "source": [
        "### 3. *XGBoost a través de W2V*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gSZRyY2JfmAu",
        "colab": {}
      },
      "source": [
        "\"CELDA N°57\"\n",
        "#Aplicamos el modelo de XGBClassifier para los datos procesados según W2V \n",
        "xgb_w2v = xgb.fit(Xtrain_w2v, ytrain_w2v) \n",
        "pred_xgb_w2v = xgb_w2v.predict(Xtest_w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_h06i0Wn17Y",
        "colab_type": "text"
      },
      "source": [
        "# **Evaluación de modelos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csGpSYkkGScG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°58\"\n",
        "#Definimos una lista de métricas y creamos una función para calcular todas las métricas\n",
        "lista_metricas=[accuracy_score,precision_score,recall_score,f1_score,roc_auc_score]\n",
        "def calcula_scores_modelo(score_modelo_nlp,real,prediccion):\n",
        "  for metrica in lista_metricas: score_modelo_nlp.append(metrica(real, prediccion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-YWMUNfMnD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°59\"\n",
        "#Calculamos todas las métricas para el modelo de Regresión Logística con cada procesamiento de NLP: BOW, TF-IDF, W2V\n",
        "score_rl_bow=[]\n",
        "calcula_scores_modelo(score_rl_bow,ytest_bow,pred_rl_bow)\n",
        "\n",
        "score_rl_tfidf=[]\n",
        "calcula_scores_modelo(score_rl_tfidf,ytest_tfidf,pred_rl_tfidf)\n",
        "\n",
        "score_rl_w2v=[]\n",
        "calcula_scores_modelo(score_rl_tfidf,ytest_w2v,pred_rl_w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzHsMmYkTd3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°60\"\n",
        "#Calculamos todas las métricas para el modelo de Random Forest con cada procesamiento de NLP: BOW, TF-IDF, W2V\n",
        "score_rf_bow=[]\n",
        "calcula_scores_modelo(score_rf_bow,ytest_bow,pred_rf_bow)\n",
        "\n",
        "score_rf_tfidf=[]\n",
        "calcula_scores_modelo(score_rf_tfidf,ytest_tfidf,pred_rf_tfidf)\n",
        "\n",
        "score_rf_w2v=[]\n",
        "calcula_scores_modelo(score_rf_w2v,ytest_w2v,pred_rf_w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQx6lFDRT6ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°61\"\n",
        "#Calculamos todas las métricas para el modelo de XGBoost con cada procesamiento de NLP: BOW, TF-IDF, W2V\n",
        "score_xgb_bow=[]\n",
        "calcula_scores_modelo(score_xgb_bow,ytest_bow,pred_xgb_bow)\n",
        "\n",
        "score_xgb_tfidf=[]\n",
        "calcula_scores_modelo(score_xgb_tfidf,ytest_tfidf,pred_xgb_tfidf)\n",
        "\n",
        "score_xgb_w2v=[]\n",
        "calcula_scores_modelo(score_xgb_w2v,ytest_w2v,pred_xgb_w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QInzVgYMyzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°62\"\n",
        "#Generamos una tabla donde se consolide todos los indicadores\n",
        "tabla_score = {'RegLogistica_BOW': score_rl_bow,'RegLogistica_TF_IDF': score_rl_tfidf, 'RegLogistica_W2V': score_rl_w2v,\n",
        "               'RandomForest_BOW': score_rf_bow,'RandomForest_TF_IDF': score_rf_tfidf, 'RandomForest_W2V': score_rf_w2v,\n",
        "               'XGBoost_BOW': score_xgb_bow,'XGBoost_TF_IDF': score_xgb_tfidf, 'XGBoost_W2V': score_xgb_w2v}\n",
        "tabla_score = pd.DataFrame(tabla_score)  \n",
        "tabla_score.index = ['accuracy_score','precision_score','recall_score','f1_score','roc_auc_score']\n",
        "tabla_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B906Nirqdd4K",
        "colab_type": "text"
      },
      "source": [
        "# **Summit de las predicciones**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GagK4cKZdH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°63\"\n",
        "#Generamos las predicciones con el mejor modelo a partir de su respectivo summit\n",
        "summit_pred = xgb_w2v.predict(summit_w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2m1dnt4bspa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°64\"\n",
        "#Extraer los Id de los pasajeros del archivo summit original\n",
        "origin = pd.read_csv('https://raw.githubusercontent.com/javalpe/datasets/master/test_twitter_analysis.csv')\n",
        "summit_id=origin['id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euzLV-BraGSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°65\"\n",
        "#Comprobar que el tamaño de las predicciones y summmit_id coincidan para unir ambos dataset sin errores\n",
        "summit_pred.shape, summit_id.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d5nXsrnbWwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°66\"\n",
        "#Crear un diccionario para guardar ambos dataset y crear un nuevo dataframe llamado respuestas para unir ambos dataset\n",
        "summit_dict = {'id':summit_id,'label':summit_pred}\n",
        "respuestas = pd.DataFrame(summit_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LiwonFlc36x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"CELDA N°67\"\n",
        "#Generar un archivo csv con tu nombre -no olvidar la extensión csv\n",
        "respuestas.to_csv('nombre_apellido.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}