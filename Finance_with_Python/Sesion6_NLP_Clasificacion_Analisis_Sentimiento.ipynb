{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zf8amewP3xeb"
   },
   "source": [
    "# **Presentación del caso: Análisis de Sentimientos con Twitter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqZ4QkZXecXk"
   },
   "source": [
    "Este caso muestra un conjunto de **13,962 tweets** en la data de entrenamiento. Cuenta con una la columna *label* (etiqueta u objetivo) donde se registra **dos valores**:\n",
    "1.   0: tweet positivo sin mención de temas políticos, racistas, etc\n",
    "2.   1: tweet negativo que menciona temas políticos, racistas, etc\n",
    "![Image of Yaktocat](https://s3.amazonaws.com/thinkific/file_uploads/118220/images/0c8/229/64d/1549269497113.jpg)\n",
    "\n",
    "Para tratar la información contenida en los tweets, **data no estructurada**, procederemos a utilizar la librería **nltk** -Natural Language Tokenization- y los siguientes modelos de NLP -Natural Language Processing para procesar los textos:\n",
    "1.   BOW: Bag of Words\n",
    "2.   TF-IDF\n",
    "3.   W2V: Word to Vec\n",
    "\n",
    "Finalmente para realizar las predicciones complementamos a los modelos de NLP antes mencionados varios modelos de clasificación para predecir la columna label por tratarse de dos categorías (0 y 1).\n",
    "\n",
    "\n",
    "Los detalles de la competencia **gratuita** pueden encontrarse en la siguiente [web](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AOSdmtwCHokE"
   },
   "source": [
    "# **Comprensión de los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rgLxZwwc598f"
   },
   "outputs": [],
   "source": [
    "\"CASO N°1\"\n",
    "#Importamos las librerías necesarias \n",
    "import pandas as pd\n",
    "import re    # regular expression nos permite editar la visualización de los caracteres según nuestras prefencias\n",
    "import nltk  # natural language tokenization es una librería para manipular las palabras que encontraremos en los tweets\n",
    "import string # exclusivamente para palabras \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_colwidth\", 200) #Definimos un máximo de ancho de columna por la naturaleza del largo de los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8o_VJey44TK4"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°2\"\n",
    "#Leemos los datos de entrenamiento -train- y los datos de testing -test\n",
    "train=pd.read_csv('https://raw.githubusercontent.com/javalpe/datasets/master/train_twitter_analysis.csv', index_col=\"id\")\n",
    "summit=pd.read_csv('https://raw.githubusercontent.com/javalpe/datasets/master/test_twitter_analysis.csv', index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3wDEKCMEJYj"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°3\"\n",
    "#Visualizamos el tamaño de cada dataset con el comando shape\n",
    "train.shape, summit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mkDrRcB57-JZ"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°4\"\n",
    "#Visualizamos el contenido de los tweets con el label = 0 que serían considerados positivos\n",
    "train[train['label'] == 0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aj5eXXYm83SH"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°5\"\n",
    "#Visualizamos el contenido de los tweets con el label = 1 que serían considerados negativos\n",
    "train[train['label'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VvBqSPsy9Iqr"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°6\"\n",
    "#Podemos distinguir la cantidad de tweets positivos y negativos con el método value_counts()\n",
    "train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HjLUIj2Q_k6l"
   },
   "source": [
    "Como la cantidad de tweets positivos es **15 veces mayor** de los tweets negativos podemos concluir que tenemos **data desbalanceada**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O4_21L6w_j97"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°7\"\n",
    "#Visualizamos el largo de los tweets -número de caracteres- en train y test con el método str.len()\n",
    "length_train = train['tweet'].str.len() \n",
    "length_summit = summit['tweet'].str.len() \n",
    "plt.hist(length_train, bins=20, label=\"train_tweets\") #el método hist de la librería matplotlib (plt) permite dibujar histogramas\n",
    "plt.hist(length_summit, bins=20, label=\"summit_tweets\") #el parámetro label es para identificar estos datos para una leyenda del gráfico\n",
    "plt.legend() #el comando legend permite añadir una leyenda a nuestra gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J-flHGjBOw5m"
   },
   "source": [
    "Adicionalmente se visualiza que la cantidad de caracteres de los tweets positivos es mucho mayor que los tweets negativos. Por ello podemos concluir que el modelo final será **más preciso para predecir tweets positivos que negativos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f7XWodKNBXoF"
   },
   "source": [
    "# **Preprocesamiento de los datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFUJZQGqBkqT"
   },
   "source": [
    "## **Extracción de los caracteres especiales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kFN0PppRAmbd"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°8\"\n",
    "#Inicialmente juntamos train y test para realizar este tratamiento para ambos dataset. Luego volveremos a dividirlos\n",
    "combi = train.append(summit, ignore_index=True) \n",
    "combi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q3JWRtA1B_UN"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°9\"\n",
    "#Definimos una función para eliminar de los tweets alguna palabra, simbolo o caracter especial (que irá en el parámetro pattern) que creamos conveniente\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Abl-jc8ODmfx"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°10\"\n",
    "#Quitamos todas aquellas menciones dentro de los tweets a otros usuarios (que comienzan con @)\n",
    "combi['tweet_limpio'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")\n",
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0IxTDBHFqPf"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°11\"\n",
    "#Reemplazamos todos aquellos carácteres especiales como signos de puntuación por un espacio en blanco\n",
    "combi['tweet_limpio'] = combi['tweet_limpio'].str.replace(\"[^a-zA-Z#]\", \" \") #el simbolo ^ signfica \"todo menos\" en este caso palabras y números \n",
    "combi.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KB255XNiGf6G"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°12\"\n",
    "#Removemos todas aquellas palabras con un largo de 3 caracteres o menos que podrían incluir conectores, advervios, artículos (por ejemplo a, and, the)\n",
    "combi['tweet_limpio'] = combi['tweet_limpio'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "combi.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAsClMSCI0mM"
   },
   "source": [
    "## **Tokenización y Steamming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dO7z6_TSI9Xf"
   },
   "source": [
    "Para poder evaluar cada tweet será necesario dividirlo en palabras, este proceso se denomina *tokenization*.\n",
    "\n",
    "También realizaremos el proceso denominado *steamming* para solo utilizar las raíces de las palabras en el análisis (por ejemplo \"read\" en vez de \"reading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6gPUqADHjR8"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°13\"\n",
    "#Dividimos los tweets en palabras y guardamos el resultado en una nueva columna denominada tweet_limpio\n",
    "tokenized_tweet = combi['tweet_limpio'].apply(lambda x: x.split()) #el comando split signfica dividir\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f9jgL_TfKA3S"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°14\"\n",
    "#Ahora procederemos a extraer solo la raíz de las palabras con el método stem.porter y actualizamos la columna tweet_limpio\n",
    "from nltk.stem.porter import * \n",
    "stemmer = PorterStemmer() \n",
    "stemmed_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0ZXUEwqKiwq"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°15\"\n",
    "#Finalmente juntamos cada raíz de palabra para almacenarla nuevamente en la columna tweet_limpio\n",
    "for i in range(len(stemmed_tweet)):\n",
    "    stemmed_tweet[i] = ' '.join(stemmed_tweet[i])    \n",
    "combi['tweet_limpio'] = stemmed_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVx12ijWL8iL"
   },
   "source": [
    "## **Aplicaciones de visualización**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XV7doBonMDx_"
   },
   "source": [
    "### 1. *WORDCLOUD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNQhuw2HUBuy"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°16\"\n",
    "#Importamos la librería WordCloud para visualizar las palabras más usadas (que aparecen en mayor tamaño)\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "To6GdGrDOFkm"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°17\"\n",
    "#Mostramos las palabras más usadas\n",
    "all_words = ' '.join([text for text in combi['tweet_limpio']])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) \n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xcf4GrYKNaNA"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°18\"\n",
    "#Mostramos las palabras más usadas en los tweets positivos\n",
    "normal_words =' '.join([text for text in combi['tweet_limpio'][combi['label'] == 0]]) \n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1aRZs-RgOmYg"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°19\"\n",
    "#Mostramos las palabras más usadas en los tweets negativos\n",
    "negative_words =' '.join([text for text in combi['tweet_limpio'][combi['label'] == 1]]) \n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K44SVviSUd20"
   },
   "source": [
    "### 2. *HASHTAG MÁS USADOS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fg34kkGKO9K9"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°20\"\n",
    "# Definimos una función que nos permita extraer los hashtags (#) de los tweets\n",
    "def hashtag_extract(x):\n",
    "    hashtags = []\n",
    "    for i in x:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lPZPRWdEovQH"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°21\"\n",
    "#Extraemos los hashtags (#) de los tweets que no son racistas ni sexistas (label=0) y guardamos el resultado en la variable HT_positivos\n",
    "HT_positivos = hashtag_extract(combi['tweet_limpio'][combi['label'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKsMwcnWozED"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°22\"\n",
    "#Extraemos los hashtags (#) de los tweets que sí son racistas ni sexistas (label=1) y guardamos el resultado en la variable HT_negativos \n",
    "HT_negativos = hashtag_extract(combi['tweet_limpio'][combi['label'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ETr9667Co1d1"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°23\"\n",
    "#Para poder realizar un conteo y graficarlo en barras procedemos a unir todas las palabras de cada variable\n",
    "HT_positivos = sum(HT_positivos,[])\n",
    "HT_negativos = sum(HT_negativos,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMYvLjxjqI2q"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°24\"\n",
    "#Con ayuda del método FreqDist realizamos el conteo y graficamos las frecuencias de aparición de cada hashtag en los tweets de la variable HT_regular (label=0)\n",
    "a = nltk.FreqDist(HT_positivos)\n",
    "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                  'Count': list(a.values())})\n",
    "d = d.nlargest(columns=\"Count\", n = 10) #Para efectos de visualización solo nos quedaremos con el top10\n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\") #Para\n",
    "ax.set(ylabel = 'Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mtX9ZK2gs5ay"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°25\"\n",
    "#Con ayuda del método FreqDist realizamos el conteo y graficamos las frecuencias de aparición de cada hashtag en los tweets de la variable HT_negative (label=1)\n",
    "b = nltk.FreqDist(HT_negativos)\n",
    "e = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n",
    "e = e.nlargest(columns=\"Count\", n = 10)   #Para efectos de visualización solo nos quedaremos con el top10\n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\n",
    "ax.set(ylabel = 'Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6T5wMKCmaMWw"
   },
   "source": [
    "## **Bag of Words (BOW)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a2lKhxkEJ1Lk"
   },
   "source": [
    "El primer método para preparar la data y poder ejecutar modelos de clasificación se denomina **Bag of Words** donde se obtiene un dataset compuesto por D documentos (que representan filas o registros) y N corpus (que representan las variables predictoras)\n",
    "\n",
    "Para nuestro ejemplo el objetivo es obtener como variables predictoras (columnas o *features*) las palabras y las filas o registros serán cada tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGjHab30Jsc2"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°26\"\n",
    "#Importamos la librería CountVectorizer de sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(analyzer='word',stop_words='english') #en nuestro caso estamos procesando tweets en inglés\n",
    "bow = bow_vectorizer.fit_transform(combi['tweet_limpio']) #guardamos el resultado en una variable denominada bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zztKR0Z9Lq6c"
   },
   "source": [
    "## **TF - IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yb0_wIoULqZR"
   },
   "source": [
    "El segundo método para preparar la data y poder ejecutar modelos de clasificación se denomina **TF - IDF** donde se obtiene un dataset conformado por los mismos elementos. Sin embargo vamos a añadir dos elementos de evaluación (como pesos para cada variable):\n",
    "\n",
    "\n",
    "1.   TF = # veces que aparece una palabra en un solo documento / # palabras totales de ese mismo documento\n",
    "2.   IDF = log (N/n) n es el # veces que aparece una palabra en cada documento y N es el # total de documentos\n",
    "\n",
    "\n",
    "Para nuestro ejemplo recordemos que un documento es un tweet y una palabra es cada una de las palabras que hemos tokenizado y steammeado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "If_A9dU2N-Q_"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°27\"\n",
    "#Importamos la librería TfidVectorizer de sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',stop_words='english') #en nuestro caso estamos procesando tweets en inglés\n",
    "tfidf = tfidf_vectorizer.fit_transform(combi['tweet_limpio']) #guardamos el resultado en una variable denominada tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKYNs5D0Q0AS"
   },
   "source": [
    "## **Word2Vec (w2v)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62ZB0EbFQ4Q-"
   },
   "source": [
    "El tercer método para preparar la data y poder ejecutar modelos de clasificación se denomina **Word2Vec** donde cada palabra se transforma en un vector para relacionar las palabras entre sí antes de generar el dataset. Esto nos trae dos ventajas:\n",
    "\n",
    "\n",
    "1.   Se reduce significativamente la cantidad de columnas generadas\n",
    "2.   Se puede distinguir dos significados distintos para una misma palabra (por ejemplo *apple* puede significar fruta y también signficar empresa tecnológica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OnK_Hmo-OfF4"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°28\"\n",
    "#Dividimos nuevamente la columna tweet_limpio palabra por palabra para poder realizar el análisis y guardamos el resultado en la variable tokenized_tweet\n",
    "tokenized_tweet = combi['tweet_limpio'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vWtEOV74UPDS"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°29\"\n",
    "#Importamos las librerías que necesitamos para ejecutar el análisis Word2vec\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XhIvVLSZT1ag"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°30\"\n",
    "#Generamos un modelo word2vec y guardamos el modelo en la variable model_w2v\n",
    "model_w2v = gensim.models.Word2Vec(tokenized_tweet,\n",
    "                                   window=5,\n",
    "                                   size= 300,\n",
    "                                   min_count=2,\n",
    "                                   alpha=0.03,\n",
    "                                   min_alpha=0.0007,\n",
    "                                   negative=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTZ5CAEBZz2N"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°31\"\n",
    "#Entrenamos el modelo con los tweets tokenizados\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(combi['tweet_limpio']), epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0tTtBYInjWMv"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°32\"\n",
    "#Definimos una función para generar un vector para cada uno de los tweet\n",
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: continue\n",
    "        if count != 0: vec/= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opRbFRDxmk6B"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°33\"\n",
    "#Aplicamos la función sobre cada tweet y generamos un dataframe que guardaremos como la variable wordvec_df\n",
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 300)) \n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 300)\n",
    "    wordvec_df = pd.DataFrame(wordvec_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S8Ij_n_Jm-ag"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°34\"\n",
    "#Comprobamos el tamaño de cada variable creada\n",
    "bow.shape, tfidf.shape, wordvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KayjIMIKe-vI"
   },
   "source": [
    "# **Modelamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H2myCykiqEAS"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°35\"\n",
    "#Dividimos la variable bow en train y summit originales\n",
    "train_bow = bow[:31962,:] #Esta parte de la data sí contiene el label\n",
    "summit_bow = bow[31962:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwCWCDcxqOAV"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°36\"\n",
    "#Dividimos la variable tfidf en train y summit originales\n",
    "train_tfidf = tfidf[:31962,:] #Esta parte de la data sí contiene el label\n",
    "summit_tfidf = tfidf[31962:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EkawJR55qWT_"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°37\"\n",
    "#Dividimos la variable wordvec_df en train y summit originales\n",
    "train_w2v = wordvec_df.iloc[:31962,:] #Esta parte de la data sí contiene el label\n",
    "summit_w2v = wordvec_df.iloc[31962:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELhvEN22g_iS"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°38\"\n",
    "#Utilizamos el método train_test_split para dividir train y test\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain_bow, Xtest_bow, ytrain_bow, ytest_bow = train_test_split(train_bow, train['label'], test_size=0.15)\n",
    "Xtrain_tfidf, Xtest_tfidf, ytrain_tfidf, ytest_tfidf = train_test_split(train_tfidf, train['label'], test_size=0.15)\n",
    "Xtrain_w2v, Xtest_w2v, ytrain_w2v, ytest_w2v = train_test_split(train_w2v, train['label'], test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzeyCes3n6RV"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°39\"\n",
    "#Importamos las librerías útiles para todos los modelos\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWOF1_KyfGdx"
   },
   "source": [
    "## **Regresión Logística**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gbb7-8FVjNRa"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°40\"\n",
    "#Importamos las librerías de Regresión Logística y guardamos el modelo de Regresión Logística en la variable lreg\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24Fd9KHHKyh6"
   },
   "source": [
    "### 1. *Regresión Logística a través de BOW*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIBP1QT8v6rJ"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°41\"\n",
    "#Entrenamos el modelo de Regresión Logística con Xtrain_bow, ytrain_bow y generamos una lista de accuracy score a partir de Xtest_bow, ytest_bow\n",
    "lreg.fit(Xtrain_bow, ytrain_bow)\n",
    "proba_rl_bow = lreg.predict_proba(Xtest_bow)[:,1]\n",
    "list_accuracy_bow=[]\n",
    "for punto_de_corte in range(0,100):\n",
    "    pred_rl_bow = [1 if x >= punto_de_corte/100 else 0 for x in proba_rl_bow]\n",
    "    list_accuracy_bow.append(accuracy_score(ytest_bow, pred_rl_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDYZENNPwiiC"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°42\"\n",
    "#Dibujamos los puntos de corte con sus respectivos score\n",
    "xs = [x/100 for x in range(0,100)]\n",
    "ys = list_accuracy_bow\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.grid(True)\n",
    "plt.xlabel('Punto de corte')\n",
    "plt.ylabel('list_accuracy_bow')\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S7EUCSz-KxCH"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°43\"\n",
    "#Actualizamos las predicciones de BOW con el punto de corte óptimo\n",
    "pred_rl_bow = proba_rl_bow >= 0.19\n",
    "pred_rl_bow = pred_rl_bow.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "as6Hb_2fLEqY"
   },
   "source": [
    "### 2. *Regresión Logística a través de TF-IDF*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oY-z7plLy2H"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°44\"\n",
    "#Entrenamos el modelo de Regresión Logística con Xtrain_tfidf, ytrain_tfidf y generamos una lista de accuracy score a partir de Xtest_tfidf, ytest_tfidf\n",
    "lreg.fit(Xtrain_tfidf, ytrain_tfidf)\n",
    "proba_rl_tfidf = lreg.predict_proba(Xtest_tfidf)[:,1]\n",
    "list_accuracy_tfidf=[]\n",
    "for punto_de_corte in range(0,100):\n",
    "    pred_rl_tfidf = [1 if x >= punto_de_corte/100 else 0 for x in proba_rl_tfidf]\n",
    "    list_accuracy_tfidf.append(accuracy_score(ytest_tfidf, pred_rl_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kd0o9-Fwz6Eu"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°45\"\n",
    "#Dibujamos los puntos de corte con sus respectivos score\n",
    "xs = [x/100 for x in range(0,100)]\n",
    "ys = list_accuracy_tfidf\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.grid(True)\n",
    "plt.xlabel('Punto de corte')\n",
    "plt.ylabel('list_accuracy_tfidf')\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rx_vkRfq0MXy"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°46\"\n",
    "#Actualizamos las predicciones de TF-IDF con el punto de corte óptimo\n",
    "pred_rl_tfidf = proba_rl_tfidf >= 0.17\n",
    "pred_rl_tfidf = pred_rl_tfidf.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LUooulQUQkQv"
   },
   "source": [
    "### 3. *Regresión Logística a través de W2V*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iyctXiPwSQXL"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°47\"\n",
    "#Entrenamos el modelo de Regresión Logística con Xtrain_w2v, ytrain_w2v y generamos una lista de accuracy score a partir de Xtest_w2v, ytest_w2v\n",
    "lreg.fit(Xtrain_w2v, ytrain_w2v)\n",
    "proba_rl_w2v = lreg.predict_proba(Xtest_w2v)[:,1]\n",
    "list_accuracy_w2v=[]\n",
    "for punto_de_corte in range(0,100):\n",
    "    pred_rl_w2v = [1 if x >= punto_de_corte/100 else 0 for x in proba_rl_w2v]\n",
    "    list_accuracy_w2v.append(accuracy_score(ytest_w2v, pred_rl_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oHomLCFEdLDX"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°48\"\n",
    "#Dibujamos los puntos de corte con sus respectivos score\n",
    "xs = [x/100 for x in range(0,100)]\n",
    "ys = list_accuracy_w2v\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.grid(True)\n",
    "plt.xlabel('Punto de corte')\n",
    "plt.ylabel('list_accuracy_w2v')\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FBc-wJI8fvTl"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°49\"\n",
    "#Actualizamos las predicciones de W2V con el punto de corte óptimo\n",
    "pred_rl_w2v = proba_rl_w2v >= 0.18\n",
    "pred_rl_w2v = pred_rl_w2v.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eUSy7Z5GXYqK"
   },
   "source": [
    "## **RandomForest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YakAW14fdQyi"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°50\"\n",
    "#Importamos la librería de RandomForestClassifier y guardamos el modelo de Random Forest en la variable rf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf=RandomForestClassifier(max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGIvcc_4kj2P"
   },
   "source": [
    "### 1. *Random Forest a través de BOW*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "66xtuAQClGzd"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°51\"\n",
    "#Aplicamos el modelo de Random Forest en Xtrain_bow, ytrain_bow y generamos las predicciones a partir de Xtest_bow\n",
    "rf_bow = rf.fit(Xtrain_bow, ytrain_bow)\n",
    "pred_rf_bow = rf_bow.predict(Xtest_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FQp2nOTZkqN0"
   },
   "source": [
    "### 2. *Random Forest a través de TF-IDF*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GggwVCHoXk8U"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°52\"\n",
    "#Aplicamos el modelo de Random Forest en Xtrain_tfidf, ytrain_tfidf y generamos las predicciones a partir de Xtest_tfidf\n",
    "rf_tfidf = rf.fit(Xtrain_tfidf, ytrain_tfidf)\n",
    "pred_rf_tfidf = rf_tfidf.predict(Xtest_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACcOn_ykk62A"
   },
   "source": [
    "### 3. *Random Forest a través de W2V*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TumbQszYYaM4"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°53\"\n",
    "#Aplicamos el modelo de Random Forest en Xtrain_w2v, ytrain_w2v y generamos las predicciones a partir de Xtest_w2v\n",
    "rf_w2v = rf.fit(Xtrain_w2v, ytrain_w2v)\n",
    "pred_rf_w2v = rf_w2v.predict(Xtest_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSQC7AXFeJd6"
   },
   "source": [
    "## **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F9Q5LOFpeMOG"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°54\"\n",
    "#Importamos la librería de XGBClassifier y guardamos el modelo de XGBoost en la variable xgb\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(learning_rate = 0.1,\n",
    "                      early_stopping_rounds = 10,\n",
    "                      min_child_weight=2,\n",
    "                      gamma=0.1,\n",
    "                      subsample=0.85,\n",
    "                      colsample_bytree=0.8,\n",
    "                      objective= 'binary:logistic') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehApw7RRndkK"
   },
   "source": [
    "### 1. *XGBoost a través de BOW*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qP8C8UpeQv5"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°55\"\n",
    "#Aplicamos el modelo de XGBClassifier para los datos procesados según BOW \n",
    "xgb_bow = xgb.fit(Xtrain_bow, ytrain_bow) \n",
    "pred_xgb_bow = xgb_bow.predict(Xtest_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E8QRN_eono4C"
   },
   "source": [
    "### 2. *XGBoost a través de TF-IDF*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMEF40ftfXVK"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°56\"\n",
    "#Aplicamos el modelo de XGBClassifier para los datos procesados según TF-IDF \n",
    "xgb_tfidf = xgb.fit(Xtrain_tfidf, ytrain_tfidf) \n",
    "pred_xgb_tfidf = xgb_tfidf.predict(Xtest_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dvU94O5inuYZ"
   },
   "source": [
    "### 3. *XGBoost a través de W2V*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSZRyY2JfmAu"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°57\"\n",
    "#Aplicamos el modelo de XGBClassifier para los datos procesados según W2V \n",
    "xgb_w2v = xgb.fit(Xtrain_w2v, ytrain_w2v) \n",
    "pred_xgb_w2v = xgb_w2v.predict(Xtest_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_h06i0Wn17Y"
   },
   "source": [
    "# **Evaluación de modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "csGpSYkkGScG"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°58\"\n",
    "#Definimos una lista de métricas y creamos una función para calcular todas las métricas\n",
    "lista_metricas=[accuracy_score,precision_score,recall_score,f1_score,roc_auc_score]\n",
    "def calcula_scores_modelo(score_modelo_nlp,real,prediccion):\n",
    "  for metrica in lista_metricas: score_modelo_nlp.append(metrica(real, prediccion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3-YWMUNfMnD5"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°59\"\n",
    "#Calculamos todas las métricas para el modelo de Regresión Logística con cada procesamiento de NLP: BOW, TF-IDF, W2V\n",
    "score_rl_bow=[]\n",
    "calcula_scores_modelo(score_rl_bow,ytest_bow,pred_rl_bow)\n",
    "\n",
    "score_rl_tfidf=[]\n",
    "calcula_scores_modelo(score_rl_tfidf,ytest_tfidf,pred_rl_tfidf)\n",
    "\n",
    "score_rl_w2v=[]\n",
    "calcula_scores_modelo(score_rl_tfidf,ytest_w2v,pred_rl_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vzHsMmYkTd3p"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°60\"\n",
    "#Calculamos todas las métricas para el modelo de Random Forest con cada procesamiento de NLP: BOW, TF-IDF, W2V\n",
    "score_rf_bow=[]\n",
    "calcula_scores_modelo(score_rf_bow,ytest_bow,pred_rf_bow)\n",
    "\n",
    "score_rf_tfidf=[]\n",
    "calcula_scores_modelo(score_rf_tfidf,ytest_tfidf,pred_rf_tfidf)\n",
    "\n",
    "score_rf_w2v=[]\n",
    "calcula_scores_modelo(score_rf_w2v,ytest_w2v,pred_rf_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQx6lFDRT6ee"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°61\"\n",
    "#Calculamos todas las métricas para el modelo de XGBoost con cada procesamiento de NLP: BOW, TF-IDF, W2V\n",
    "score_xgb_bow=[]\n",
    "calcula_scores_modelo(score_xgb_bow,ytest_bow,pred_xgb_bow)\n",
    "\n",
    "score_xgb_tfidf=[]\n",
    "calcula_scores_modelo(score_xgb_tfidf,ytest_tfidf,pred_xgb_tfidf)\n",
    "\n",
    "score_xgb_w2v=[]\n",
    "calcula_scores_modelo(score_xgb_w2v,ytest_w2v,pred_xgb_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7QInzVgYMyzC"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°62\"\n",
    "#Generamos una tabla donde se consolide todos los indicadores\n",
    "tabla_score = {'RegLogistica_BOW': score_rl_bow,'RegLogistica_TF_IDF': score_rl_tfidf, 'RegLogistica_W2V': score_rl_w2v,\n",
    "               'RandomForest_BOW': score_rf_bow,'RandomForest_TF_IDF': score_rf_tfidf, 'RandomForest_W2V': score_rf_w2v,\n",
    "               'XGBoost_BOW': score_xgb_bow,'XGBoost_TF_IDF': score_xgb_tfidf, 'XGBoost_W2V': score_xgb_w2v}\n",
    "tabla_score = pd.DataFrame(tabla_score)  \n",
    "tabla_score.index = ['accuracy_score','precision_score','recall_score','f1_score','roc_auc_score']\n",
    "tabla_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B906Nirqdd4K"
   },
   "source": [
    "# **Summit de las predicciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0GagK4cKZdH0"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°63\"\n",
    "#Generamos las predicciones con el mejor modelo a partir de su respectivo summit\n",
    "summit_pred = xgb_w2v.predict(summit_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2m1dnt4bspa"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°64\"\n",
    "#Extraer los Id de los pasajeros del archivo summit original\n",
    "origin = pd.read_csv('https://raw.githubusercontent.com/javalpe/datasets/master/test_twitter_analysis.csv')\n",
    "summit_id=origin['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euzLV-BraGSV"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°65\"\n",
    "#Comprobar que el tamaño de las predicciones y summmit_id coincidan para unir ambos dataset sin errores\n",
    "summit_pred.shape, summit_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3d5nXsrnbWwU"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°66\"\n",
    "#Crear un diccionario para guardar ambos dataset y crear un nuevo dataframe llamado respuestas para unir ambos dataset\n",
    "summit_dict = {'id':summit_id,'label':summit_pred}\n",
    "respuestas = pd.DataFrame(summit_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LiwonFlc36x"
   },
   "outputs": [],
   "source": [
    "\"CELDA N°67\"\n",
    "#Generar un archivo csv con tu nombre -no olvidar la extensión csv\n",
    "respuestas.to_csv('nombre_apellido.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sesion6_NLP_Clasificacion_Analisis_Sentimiento.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
