{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistillBert - Sentiment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucqmqjf5zvLS"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHqSJsarzbkw"
      },
      "source": [
        "__author__ = \"@PBenavides\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers as ppb #Pytorch transformers\n",
        "import re\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  dev = \"cuda:0\"\n",
        "else:\n",
        "  dev = \"cpu\"\n",
        "\n",
        "device = torch.device(dev)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_d0kYr_0Vq8",
        "outputId": "a7273106-24e0-4c13-c844-33e5e88b8acd"
      },
      "source": [
        "train = pd.read_csv('https://raw.githubusercontent.com/HackSpacePeru/Datasets_intro_Data_Science/master/Nlp_twitter/train_twitter_analysis.csv').iloc[:,1:]\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/HackSpacePeru/Datasets_intro_Data_Science/master/Nlp_twitter/test_twitter_analysis.csv').iloc[:,1:]\n",
        "df = train.append(test,ignore_index=True)\n",
        "print(train.shape, test.shape, df.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31962, 2) (17197, 1) (49159, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRW0jG9O0jh_"
      },
      "source": [
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_fmNH-vSZ3c"
      },
      "source": [
        "train['tweet'] = train['tweet'].str.replace(\"@user\",\"\")\n",
        "test['tweet'] = test['tweet'].str.replace(\"@user\",\"\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUIW6qpe2bJO"
      },
      "source": [
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights).to(device) #Estamos moviendo este modelo al GPU para poder procesar los datos."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BsQI0-g4s7P"
      },
      "source": [
        "#Ahora procederemos a tokenizar el dataset. En este caso procesaremos todas las oraciones en una, como un batch. \n",
        "tokenized_train = train['tweet'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "tokenized_test = test['tweet'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3RNU_yAUPCP"
      },
      "source": [
        "### Padding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNxMll-65CU0",
        "outputId": "ec4c3742-d6e8-42d5-87c4-ba7131c0d52f"
      },
      "source": [
        "#Para poder ver cuál es el tamaño máximo de palabras, correremos lo siguiente\n",
        "max_len = 0\n",
        "for i in tokenized_train.values:\n",
        "  if len(i) > max_len:\n",
        "    max_len = len(i)\n",
        "print(max_len)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w99A5KN05Oj9",
        "outputId": "0fa71271-926a-445d-b330-15b4dc7f6a7b"
      },
      "source": [
        "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized_train.values])\n",
        "print(np.array(padded).shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31962, 137)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3EEWGfnr5kx"
      },
      "source": [
        "### Masking\n",
        "Si metemos directamente el objeto padded a Bert, lo confundirá un poco. Necesitamos crear otra variable para decirle que ignore el padding que agregamos cuando esté procesando el input. Es decir, que si hay un tweet con menos tamaño del max_len, que ponga un 0 en esa posición. Esto es como usar un máscara de atención, así que le pondremos attention_mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLAR6aRyWukM",
        "outputId": "fada9c64-92ff-4ac0-aa35-0d95918f2c54"
      },
      "source": [
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "attention_mask.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31962, 137)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6PkgR7AskOc"
      },
      "source": [
        "Ahora procederemos a transformas las oraciones a los embeddings que tanto queremos con el Algoritmo DistilBERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDnF0AVzvb3k",
        "outputId": "1918409a-8a5b-4aac-97c4-f540c20a19b7"
      },
      "source": [
        "#Cuando los convierto a tensores, estaré llevándolos luego al GPU para poder hacer el procesamiento.\n",
        "input_ids = torch.tensor(padded).to(device)\n",
        "attention_mask = torch.tensor(attention_mask).to(device)\n",
        "print(input_ids.shape, attention_mask.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([31962, 137]) torch.Size([31962, 137])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X0F6xgmSzOs"
      },
      "source": [
        "Dado que Colab solo nos da 14Gbs de GPU, tendremos que acortar los 31mil tweets de train a solo 10mil, esto hará que tengamos una representación vectorial de esos 10mil tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duR7ftuWsQqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5738c317-564d-48ce-b70e-389dd7b920d3"
      },
      "source": [
        "batch_step = 100\n",
        "train_size = 12500 #Para elegir un número limitado de tweets, así haremos la prueba pues la memoria del GPU revienta.\n",
        "\n",
        "import gc\n",
        "embedding_tensor = torch.empty(size=(0,137,768)).to(device)\n",
        "\n",
        "#El GPU de Colab no soporta tantos datos, recuerda que estamos hablando de 32000*137*768\n",
        "with torch.no_grad():\n",
        "  for batch_ in [(i,i+batch_step) for i in range(0,train_size,batch_step)]:\n",
        "\n",
        "    last_hidden_states = model(input_ids[batch_[0]:batch_[1]])\n",
        "    embedding_tensor = torch.cat([embedding_tensor, last_hidden_states[0]],dim=0)\n",
        "\n",
        "    del last_hidden_states\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"El embedding representa {} tweets con un máximo de {} palabras por tweet en {} números\".format(embedding_tensor.shape[0], embedding_tensor.shape[1], embedding_tensor.shape[2]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El embedding representa 12500 tweets con un máximo de 137 palabras por tweet en 768 números\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16f_aLJmTYdg"
      },
      "source": [
        "#### Sobre el embedding:\n",
        "\n",
        "Los embeddings en BERT tienen la característica de darnos un clasificador como primer número representativo. Esto se puede usar para clasificar luego nuestros tweets. Así que lo guardaremos en una variable **features**. Recuerda que este CLS nos sirve como representación total de cada doc que estamos poniendo en BERT. Es como una forma de reducir las dimensiones para poder tenerlo en una especie de tabla y hacer luego las predicciones con una regresión Logística."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma8UZdcetVox",
        "outputId": "d050076c-d502-4aca-9f63-ed15c19d2677"
      },
      "source": [
        "features = embedding_tensor[:,0,:].cpu().numpy()\n",
        "labels = train['label'].iloc[0:train_size]\n",
        "print(features.shape, labels.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12500, 768) (12500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIFRUcfgV-BZ"
      },
      "source": [
        "### Modelo de clasificación.\n",
        "\n",
        "A partir de acá, ya tenemos los features. Solo nos concetraremos en mejorar el modelo para poder tener la mejor clasificación. En este caso, haremos un modelo logístico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS1VppP-7OdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0924f092-d515-4ea3-ef1e-a3a8291dc7ae"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
        "pipeline = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
        "\n",
        "pipeline.fit(X_train, y_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('standardscaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('logisticregression',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=1000,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DiM0AtwDxFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69c9b9e0-770d-4f23-8569-53b691b3f6e8"
      },
      "source": [
        "print(pipeline.score(X_test, y_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.95104\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}