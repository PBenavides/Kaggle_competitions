{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def remove_stopword(x, lista_stopwords):\n",
    "    return [y for y in x if y not in lista_stopwords]\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers. Also, we added the unicode line for accent marks'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text) #Punctuations...\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    #text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    #text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "def make_clean_dataframe(stopwords_espaniol, path_textos):\n",
    "    #Accedo al path y jalo toda la info\n",
    "    dicc={}\n",
    "    for nombre_doc in os.listdir(path_textos):\n",
    "        text_string = open(path_textos+'/'+nombre_doc).read()\n",
    "        dicc[nombre_doc[:-4]] = text_string\n",
    "    \n",
    "    #Limpio y transformo el texto.\n",
    "    dataframe = pd.DataFrame(dicc,index=[0]).T.rename(columns={0:'texto'}).reset_index()\n",
    "    dataframe['temp_list'] = dataframe['texto'].apply(lambda x: clean_text(x))\n",
    "    dataframe['temp_list'] = dataframe['temp_list'].apply(lambda x: str(x).split())\n",
    "    dataframe['texto_limpio'] = dataframe['temp_list'].apply(lambda x: remove_stopword(x, stopwords_espaniol))\n",
    "    dataframe = dataframe.rename(columns={'index':'nombre_doc'})\n",
    "\n",
    "    for k,v in dataframe['texto_limpio'].items():\n",
    "        dataframe.loc[k,'raw_clean_text'] = ' '.join(dataframe.loc[k,'texto_limpio'])\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "agg_stopword = ['s', '2018','31','diciembre','financieros','000','2019','nota','grupo','valor','2017','resultados','compania','1',\n",
    " 'total','consolidados','consolidado','razonable','gerencia','ciento','c','activos','cuentas','neto','us','efectivo','fecha','peru',\n",
    " 'inretail','2','3','importe', 'aproximadamente','b','respectivamente','ver','ano','si','vida','anos','4','d','5','i','www','com',\n",
    " 'aa', 'aaa', 'aaahipotecario', 'aaatat', 'aamnto', 'ab','ir','email','mes','niif','fmiv','bbb','ok','mzo','inc','alicorp','notas','dic']\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords_espaniol = stopwords.words('spanish')\n",
    "stopwords_espaniol.extend(agg_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre_doc</th>\n",
       "      <th>texto</th>\n",
       "      <th>temp_list</th>\n",
       "      <th>texto_limpio</th>\n",
       "      <th>raw_clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NOTAS_ALICORP_2018_1Q</td>\n",
       "      <td>alicorp s a a notas a los estados financieros ...</td>\n",
       "      <td>[alicorp, s, a, a, notas, a, los, estados, fin...</td>\n",
       "      <td>[marzo, expresados, miles, soles, principios, ...</td>\n",
       "      <td>marzo expresados miles soles principios practi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOTAS_ALICORP_2018_2Q</td>\n",
       "      <td>alicorp s a a notas a los estados financieros ...</td>\n",
       "      <td>[alicorp, s, a, a, notas, a, los, estados, fin...</td>\n",
       "      <td>[junio, expresados, miles, soles, principios, ...</td>\n",
       "      <td>junio expresados miles soles principios practi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOTAS_ALICORP_2018_3Q</td>\n",
       "      <td>alicorp s a a notas a los estados financieros ...</td>\n",
       "      <td>[alicorp, s, a, a, notas, a, los, estados, fin...</td>\n",
       "      <td>[setiembre, expresados, miles, soles, principi...</td>\n",
       "      <td>setiembre expresados miles soles principios pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOTAS_ALICORP_2018_4Q</td>\n",
       "      <td>alicorp s a a notas a los estados financieros ...</td>\n",
       "      <td>[alicorp, s, a, a, notas, a, los, estados, fin...</td>\n",
       "      <td>[expresados, miles, soles, principios, practic...</td>\n",
       "      <td>expresados miles soles principios practicas co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOTAS_ALICORP_2019_1Q</td>\n",
       "      <td>alicorp s a a notas a los estados financieros ...</td>\n",
       "      <td>[alicorp, s, a, a, notas, a, los, estados, fin...</td>\n",
       "      <td>[marzo, expresados, miles, soles, principios, ...</td>\n",
       "      <td>marzo expresados miles soles principios practi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NOTAS_ALICORP_2019_2Q</td>\n",
       "      <td>alicorp s a a notas a los estados financieros ...</td>\n",
       "      <td>[alicorp, s, a, a, notas, a, los, estados, fin...</td>\n",
       "      <td>[separados, junio, expresados, miles, soles, p...</td>\n",
       "      <td>separados junio expresados miles soles princip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NOTAS_ALICORP_2019_3Q</td>\n",
       "      <td>alicorp s a a notas a los estados financieros ...</td>\n",
       "      <td>[alicorp, s, a, a, notas, a, los, estados, fin...</td>\n",
       "      <td>[separados, septiembre, expresados, miles, sol...</td>\n",
       "      <td>separados septiembre expresados miles soles pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NOTAS_ALICORP_2019_4Q</td>\n",
       "      <td>alicorp s a a y subsidiarias notas a los estad...</td>\n",
       "      <td>[alicorp, s, a, a, y, subsidiarias, notas, a, ...</td>\n",
       "      <td>[subsidiarias, expresados, miles, soles, princ...</td>\n",
       "      <td>subsidiarias expresados miles soles principios...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NOTAS_ALICORP_2020_1Q</td>\n",
       "      <td>alicorp s a a y subsidiarias notas a los estad...</td>\n",
       "      <td>[alicorp, s, a, a, y, subsidiarias, notas, a, ...</td>\n",
       "      <td>[subsidiarias, marzo, expresados, miles, soles...</td>\n",
       "      <td>subsidiarias marzo expresados miles soles acti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              nombre_doc                                              texto  \\\n",
       "0  NOTAS_ALICORP_2018_1Q  alicorp s a a notas a los estados financieros ...   \n",
       "1  NOTAS_ALICORP_2018_2Q  alicorp s a a notas a los estados financieros ...   \n",
       "2  NOTAS_ALICORP_2018_3Q  alicorp s a a notas a los estados financieros ...   \n",
       "3  NOTAS_ALICORP_2018_4Q  alicorp s a a notas a los estados financieros ...   \n",
       "4  NOTAS_ALICORP_2019_1Q  alicorp s a a notas a los estados financieros ...   \n",
       "5  NOTAS_ALICORP_2019_2Q  alicorp s a a notas a los estados financieros ...   \n",
       "6  NOTAS_ALICORP_2019_3Q  alicorp s a a notas a los estados financieros ...   \n",
       "7  NOTAS_ALICORP_2019_4Q  alicorp s a a y subsidiarias notas a los estad...   \n",
       "8  NOTAS_ALICORP_2020_1Q  alicorp s a a y subsidiarias notas a los estad...   \n",
       "\n",
       "                                           temp_list  \\\n",
       "0  [alicorp, s, a, a, notas, a, los, estados, fin...   \n",
       "1  [alicorp, s, a, a, notas, a, los, estados, fin...   \n",
       "2  [alicorp, s, a, a, notas, a, los, estados, fin...   \n",
       "3  [alicorp, s, a, a, notas, a, los, estados, fin...   \n",
       "4  [alicorp, s, a, a, notas, a, los, estados, fin...   \n",
       "5  [alicorp, s, a, a, notas, a, los, estados, fin...   \n",
       "6  [alicorp, s, a, a, notas, a, los, estados, fin...   \n",
       "7  [alicorp, s, a, a, y, subsidiarias, notas, a, ...   \n",
       "8  [alicorp, s, a, a, y, subsidiarias, notas, a, ...   \n",
       "\n",
       "                                        texto_limpio  \\\n",
       "0  [marzo, expresados, miles, soles, principios, ...   \n",
       "1  [junio, expresados, miles, soles, principios, ...   \n",
       "2  [setiembre, expresados, miles, soles, principi...   \n",
       "3  [expresados, miles, soles, principios, practic...   \n",
       "4  [marzo, expresados, miles, soles, principios, ...   \n",
       "5  [separados, junio, expresados, miles, soles, p...   \n",
       "6  [separados, septiembre, expresados, miles, sol...   \n",
       "7  [subsidiarias, expresados, miles, soles, princ...   \n",
       "8  [subsidiarias, marzo, expresados, miles, soles...   \n",
       "\n",
       "                                      raw_clean_text  \n",
       "0  marzo expresados miles soles principios practi...  \n",
       "1  junio expresados miles soles principios practi...  \n",
       "2  setiembre expresados miles soles principios pr...  \n",
       "3  expresados miles soles principios practicas co...  \n",
       "4  marzo expresados miles soles principios practi...  \n",
       "5  separados junio expresados miles soles princip...  \n",
       "6  separados septiembre expresados miles soles pr...  \n",
       "7  subsidiarias expresados miles soles principios...  \n",
       "8  subsidiarias marzo expresados miles soles acti...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = make_clean_dataframe(stopwords_espaniol, 'data')\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hasta acá tenemos 9 estados financieros de una misma empresa ya limpios de caracteres y palabras basura. Ahora tenemos que Lemmatizar las palabras, es decir, volverlas a su raíz para mejor procesamiento. Luego, intentaremos clasificar los documentos según el modelo No Supervisado: Latent Dirichlet Allocation.\n",
    "\n",
    "Recuerda en que la diferencia entre Lemmatizer y Stemmer yace en la metodología de la reducción de palabras. Mientras Lemmatizer se basa en un análisis morfológico de las palabras y requiere de un diccionario de especificación, Stemmer se basa en cortar prefijos y sufijos comunes a las palabras involucradas en el texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def porter_stemmer(word):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [marzo, expresados, miles, soles, principios, ...\n",
       "1    [junio, expresados, miles, soles, principios, ...\n",
       "2    [setiembre, expresados, miles, soles, principi...\n",
       "3    [expresados, miles, soles, principios, practic...\n",
       "4    [marzo, expresados, miles, soles, principios, ...\n",
       "5    [separados, junio, expresados, miles, soles, p...\n",
       "6    [separados, septiembre, expresados, miles, sol...\n",
       "7    [subsidiarias, expresados, miles, soles, princ...\n",
       "8    [subsidiarias, marzo, expresados, miles, soles...\n",
       "Name: raw_clean_text, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['raw_clean_text'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subsidiaria'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer('subsidiarias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = {k:v.split() for k, v in zip(dataframe['nombre_doc'],dataframe['raw_clean_text'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean = [*dataframe['raw_clean_text'].str.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#Debemos crear un diccionario de nuestro corpus (lista de docs), donde cada término único sea asignado a un index. \n",
    "\n",
    "#Convirtiendo la lista de documentos corpus en una Matriz de términos de documentos, usando el diccionario dict_data\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(doc_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora correremos el modelo LDA que gensim nos provee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "ldamodel = LDA(doc_term_matrix, num_topics=5, id2word=dictionary, passes=40,random_state=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.012*\"arrendamiento\" + 0.011*\"activo\" + 0.011*\"inversiones\" + 0.010*\"arrendamientos\" + 0.010*\"pasivo\" + 0.010*\"derecho\"'), (1, '0.017*\"arrendamiento\" + 0.013*\"bonos\" + 0.013*\"emision\" + 0.009*\"subsidiarias\" + 0.008*\"vigentes\" + 0.008*\"bolivianos\"'), (2, '0.011*\"instrumentos\" + 0.010*\"inversiones\" + 0.009*\"ingresos\" + 0.009*\"contables\" + 0.009*\"tiempo\" + 0.009*\"cobertura\"'), (3, '0.001*\"inversiones\" + 0.001*\"emision\" + 0.001*\"marzo\" + 0.001*\"enero\" + 0.001*\"serie\" + 0.001*\"contables\"'), (4, '0.001*\"contables\" + 0.001*\"emision\" + 0.001*\"informacion\" + 0.001*\"inversiones\" + 0.001*\"serie\" + 0.001*\"gastos\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=5, num_words=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métricas del modelo: \n",
    "\n",
    "Perplexity: Qué tan bueno es el modelo con respecto a palabras buenas.\n",
    "\n",
    "Coherence Score: Qué tanto se sustenta el modelo con respecto a las palabras aledañas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -6.5309116418709365\n",
      "Coherence Score: 0.28504458324472415\n"
     ]
    }
   ],
   "source": [
    "print('Perplexity: {}'.format(ldamodel.log_perplexity(doc_term_matrix))) #Mientras más bajo mejor.\n",
    "\n",
    "#Cohere Score:\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts= doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print('Coherence Score: {}'.format(coherence_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.2686555407995678\n"
     ]
    }
   ],
   "source": [
    "ldamodel2 = LDA(doc_term_matrix, num_topics=3, id2word=dictionary, passes=60,random_state=300)\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel2, texts= doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print('Coherence Score: {}'.format(coherence_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.36828379358002655\n"
     ]
    }
   ],
   "source": [
    "#Ahora le pondremos 600 passes\n",
    "ldamodel2 = LDA(doc_term_matrix, num_topics=7, id2word=dictionary, passes=600,random_state=300)\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel2, texts= doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print('Coherence Score: {}'.format(coherence_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, '0.001*\"construcciones\" + 0.001*\"mediante\" + 0.001*\"consorcio\" + 0.001*\"considera\" + 0.001*\"financiado\" + 0.001*\"computo\"'), (0, '0.013*\"arrendamiento\" + 0.011*\"activo\" + 0.011*\"inversiones\" + 0.010*\"arrendamientos\" + 0.010*\"pasivo\" + 0.010*\"derecho\"'), (2, '0.011*\"instrumentos\" + 0.010*\"inversiones\" + 0.010*\"ingresos\" + 0.010*\"contables\" + 0.010*\"tiempo\" + 0.010*\"cobertura\"'), (5, '0.018*\"arrendamiento\" + 0.014*\"bonos\" + 0.014*\"emision\" + 0.010*\"subsidiarias\" + 0.009*\"vigentes\" + 0.009*\"bolivianos\"'), (3, '0.001*\"construcciones\" + 0.001*\"mediante\" + 0.001*\"consorcio\" + 0.001*\"considera\" + 0.001*\"financiado\" + 0.001*\"computo\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel2.print_topics(num_topics=5, num_words=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que algunos de las palabras dentro de los tópicos nos pueden ser útiles. Pero, quedan las preguntas:\n",
    "\n",
    "¿Cómo sé cuántos tópicos son óptimos dentro de mis datos? \n",
    "\n",
    "¿Cómo veo el puntaje de un tópico por documento?\n",
    "\n",
    "#### Optimizando el Número de tópicos: \n",
    "\n",
    "Fuente: https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                          | 0/540 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-348f24f53dbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m                     \u001b[1;31m# get the coherence score for the given parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary, \n\u001b[1;32m---> 59\u001b[1;33m                                                   a=a, b=b)\n\u001b[0m\u001b[0;32m     60\u001b[0m                     \u001b[1;31m# Save the model results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[0mmodel_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Validation_Set'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_title\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-348f24f53dbc>\u001b[0m in \u001b[0;36mcompute_coherence_values\u001b[1;34m(corpus, dictionary, a, b)\u001b[0m\n\u001b[0;32m     11\u001b[0m                                            \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                            \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                                            eta=b)\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mcoherence_model_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_lemmatized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c_v'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m             \u001b[0mminimum_phi_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m         )\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[1;31m# wait for all outstanding jobs to finish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mqueue_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                 \u001b[0mprocess_result_queue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreallen\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36mprocess_result_queue\u001b[1;34m(force)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \"\"\"\n\u001b[0;32m    267\u001b[0m             \u001b[0mmerged_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m                 \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[0mqueue_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36mempty\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             if (self._got_empty_message or\n\u001b[1;32m--> 328\u001b[1;33m                         _winapi.PeekNamedPipe(self._handle)[0] != 0):\n\u001b[0m\u001b[0;32m    329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def compute_coherence_values(corpus, dictionary, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=600,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "# Rango de tópicos\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "# Rango de optimización del alpha\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "# Rango de optimización del beta\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "# VTest de validación\n",
    "num_of_docs = len(doc_clean)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               gensim.utils.ClippedCorpus(doc_clean, int(num_of_docs*0.75)), \n",
    "               doc_clean]\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    # iterate through validation corpuses\n",
    "    for i in tqdm(range(len(corpus_sets))):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary, \n",
    "                                                  a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuentes a considerar:\n",
    "\n",
    "https://www.youtube.com/watch?v=T05t-SqKArY\n",
    "\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "\n",
    "https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3\n",
    "\n",
    "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "\n",
    "https://www.aclweb.org/anthology/D12-1087/\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "\n",
    "To-Supervised:\n",
    "\n",
    "https://towardsdatascience.com/unsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
