{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def remove_stopword(x, lista_stopwords):\n",
    "    return [y for y in x if y not in lista_stopwords]\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers. Also, we added the unicode line for accent marks'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text) #Punctuations...\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    #text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    #text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "def make_clean_dataframe(stopwords_espaniol, path_textos):\n",
    "    #Accedo al path y jalo toda la info\n",
    "    dicc={}\n",
    "    for nombre_doc in os.listdir(path_textos):\n",
    "        text_string = open(path_textos+'/'+nombre_doc).read()\n",
    "        dicc[nombre_doc[:-4]] = text_string\n",
    "    \n",
    "    #Limpio y transformo el texto.\n",
    "    dataframe = pd.DataFrame(dicc,index=[0]).T.rename(columns={0:'texto'}).reset_index()\n",
    "    dataframe['temp_list'] = dataframe['texto'].apply(lambda x: clean_text(x))\n",
    "    dataframe['temp_list'] = dataframe['temp_list'].apply(lambda x: str(x).split())\n",
    "    dataframe['texto_limpio'] = dataframe['temp_list'].apply(lambda x: remove_stopword(x, stopwords_espaniol))\n",
    "    dataframe = dataframe.rename(columns={'index':'nombre_doc'})\n",
    "\n",
    "    for k,v in dataframe['texto_limpio'].items():\n",
    "        dataframe.loc[k,'raw_clean_text'] = ' '.join(dataframe.loc[k,'texto_limpio'])\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "agg_stopword = ['s', '2018','31','diciembre','financieros','000','2019','nota','grupo','valor','2017','resultados','compania','1',\n",
    " 'total','consolidados','consolidado','razonable','gerencia','ciento','c','activos','cuentas','neto','us','efectivo','fecha','peru',\n",
    " 'inretail','2','3','importe', 'aproximadamente','b','respectivamente','ver','ano','si','vida','anos','4','d','5','i','www','com',\n",
    " 'aa', 'aaa', 'aaahipotecario', 'aaatat', 'aamnto', 'ab','ir','email','mes','niif','fmiv','bbb','ok','mzo','inc','alicorp','notas','dic']\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords_espaniol = stopwords.words('spanish')\n",
    "stopwords_espaniol.extend(agg_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre_doc</th>\n",
       "      <th>texto</th>\n",
       "      <th>temp_list</th>\n",
       "      <th>texto_limpio</th>\n",
       "      <th>raw_clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NOTAS_ALICORP_2018_1Q</td>\n",
       "      <td>alicorp s a a notas a los estados financieros ...</td>\n",
       "      <td>[alicorp, s, a, a, notas, a, los, estados, fin...</td>\n",
       "      <td>[marzo, expresados, miles, soles, principios, ...</td>\n",
       "      <td>marzo expresados miles soles principios practi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOTAS_ALICORP_2018_2Q</td>\n",
       "      <td>alicorp s a a notas a los estados financieros ...</td>\n",
       "      <td>[alicorp, s, a, a, notas, a, los, estados, fin...</td>\n",
       "      <td>[junio, expresados, miles, soles, principios, ...</td>\n",
       "      <td>junio expresados miles soles principios practi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOTAS_ALICORP_2018_3Q</td>\n",
       "      <td>alicorp s a a notas a los estados financieros ...</td>\n",
       "      <td>[alicorp, s, a, a, notas, a, los, estados, fin...</td>\n",
       "      <td>[setiembre, expresados, miles, soles, principi...</td>\n",
       "      <td>setiembre expresados miles soles principios pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOTAS_ALICORP_2018_4Q</td>\n",
       "      <td>alicorp s a a notas a los estados financieros ...</td>\n",
       "      <td>[alicorp, s, a, a, notas, a, los, estados, fin...</td>\n",
       "      <td>[expresados, miles, soles, principios, practic...</td>\n",
       "      <td>expresados miles soles principios practicas co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOTAS_ALICORP_2019_1Q</td>\n",
       "      <td>alicorp s a a notas a los estados financieros ...</td>\n",
       "      <td>[alicorp, s, a, a, notas, a, los, estados, fin...</td>\n",
       "      <td>[marzo, expresados, miles, soles, principios, ...</td>\n",
       "      <td>marzo expresados miles soles principios practi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NOTAS_ALICORP_2019_2Q</td>\n",
       "      <td>alicorp s a a notas a los estados financieros ...</td>\n",
       "      <td>[alicorp, s, a, a, notas, a, los, estados, fin...</td>\n",
       "      <td>[separados, junio, expresados, miles, soles, p...</td>\n",
       "      <td>separados junio expresados miles soles princip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NOTAS_ALICORP_2019_3Q</td>\n",
       "      <td>alicorp s a a notas a los estados financieros ...</td>\n",
       "      <td>[alicorp, s, a, a, notas, a, los, estados, fin...</td>\n",
       "      <td>[separados, septiembre, expresados, miles, sol...</td>\n",
       "      <td>separados septiembre expresados miles soles pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NOTAS_ALICORP_2019_4Q</td>\n",
       "      <td>alicorp s a a y subsidiarias notas a los estad...</td>\n",
       "      <td>[alicorp, s, a, a, y, subsidiarias, notas, a, ...</td>\n",
       "      <td>[subsidiarias, expresados, miles, soles, princ...</td>\n",
       "      <td>subsidiarias expresados miles soles principios...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NOTAS_ALICORP_2020_1Q</td>\n",
       "      <td>alicorp s a a y subsidiarias notas a los estad...</td>\n",
       "      <td>[alicorp, s, a, a, y, subsidiarias, notas, a, ...</td>\n",
       "      <td>[subsidiarias, marzo, expresados, miles, soles...</td>\n",
       "      <td>subsidiarias marzo expresados miles soles acti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              nombre_doc                                              texto  \\\n",
       "0  NOTAS_ALICORP_2018_1Q  alicorp s a a notas a los estados financieros ...   \n",
       "1  NOTAS_ALICORP_2018_2Q  alicorp s a a notas a los estados financieros ...   \n",
       "2  NOTAS_ALICORP_2018_3Q  alicorp s a a notas a los estados financieros ...   \n",
       "3  NOTAS_ALICORP_2018_4Q  alicorp s a a notas a los estados financieros ...   \n",
       "4  NOTAS_ALICORP_2019_1Q  alicorp s a a notas a los estados financieros ...   \n",
       "5  NOTAS_ALICORP_2019_2Q  alicorp s a a notas a los estados financieros ...   \n",
       "6  NOTAS_ALICORP_2019_3Q  alicorp s a a notas a los estados financieros ...   \n",
       "7  NOTAS_ALICORP_2019_4Q  alicorp s a a y subsidiarias notas a los estad...   \n",
       "8  NOTAS_ALICORP_2020_1Q  alicorp s a a y subsidiarias notas a los estad...   \n",
       "\n",
       "                                           temp_list  \\\n",
       "0  [alicorp, s, a, a, notas, a, los, estados, fin...   \n",
       "1  [alicorp, s, a, a, notas, a, los, estados, fin...   \n",
       "2  [alicorp, s, a, a, notas, a, los, estados, fin...   \n",
       "3  [alicorp, s, a, a, notas, a, los, estados, fin...   \n",
       "4  [alicorp, s, a, a, notas, a, los, estados, fin...   \n",
       "5  [alicorp, s, a, a, notas, a, los, estados, fin...   \n",
       "6  [alicorp, s, a, a, notas, a, los, estados, fin...   \n",
       "7  [alicorp, s, a, a, y, subsidiarias, notas, a, ...   \n",
       "8  [alicorp, s, a, a, y, subsidiarias, notas, a, ...   \n",
       "\n",
       "                                        texto_limpio  \\\n",
       "0  [marzo, expresados, miles, soles, principios, ...   \n",
       "1  [junio, expresados, miles, soles, principios, ...   \n",
       "2  [setiembre, expresados, miles, soles, principi...   \n",
       "3  [expresados, miles, soles, principios, practic...   \n",
       "4  [marzo, expresados, miles, soles, principios, ...   \n",
       "5  [separados, junio, expresados, miles, soles, p...   \n",
       "6  [separados, septiembre, expresados, miles, sol...   \n",
       "7  [subsidiarias, expresados, miles, soles, princ...   \n",
       "8  [subsidiarias, marzo, expresados, miles, soles...   \n",
       "\n",
       "                                      raw_clean_text  \n",
       "0  marzo expresados miles soles principios practi...  \n",
       "1  junio expresados miles soles principios practi...  \n",
       "2  setiembre expresados miles soles principios pr...  \n",
       "3  expresados miles soles principios practicas co...  \n",
       "4  marzo expresados miles soles principios practi...  \n",
       "5  separados junio expresados miles soles princip...  \n",
       "6  separados septiembre expresados miles soles pr...  \n",
       "7  subsidiarias expresados miles soles principios...  \n",
       "8  subsidiarias marzo expresados miles soles acti...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = make_clean_dataframe(stopwords_espaniol, 'data')\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hasta acá tenemos 9 estados financieros de una misma empresa ya limpios de caracteres y palabras basura. Ahora tenemos que Lemmatizar las palabras, es decir, volverlas a su raíz para mejor procesamiento. Luego, intentaremos clasificar los documentos según el modelo No Supervisado: Latent Dirichlet Allocation.\n",
    "\n",
    "Recuerda en que la diferencia entre Lemmatizer y Stemmer yace en la metodología de la reducción de palabras. Mientras Lemmatizer se basa en un análisis morfológico de las palabras y requiere de un diccionario de especificación, Stemmer se basa en cortar prefijos y sufijos comunes a las palabras involucradas en el texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def porter_stemmer(word):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [marzo, expresados, miles, soles, principios, ...\n",
       "1    [junio, expresados, miles, soles, principios, ...\n",
       "2    [setiembre, expresados, miles, soles, principi...\n",
       "3    [expresados, miles, soles, principios, practic...\n",
       "4    [marzo, expresados, miles, soles, principios, ...\n",
       "5    [separados, junio, expresados, miles, soles, p...\n",
       "6    [separados, septiembre, expresados, miles, sol...\n",
       "7    [subsidiarias, expresados, miles, soles, princ...\n",
       "8    [subsidiarias, marzo, expresados, miles, soles...\n",
       "Name: raw_clean_text, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['raw_clean_text'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subsidiaria'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer('subsidiarias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = {k:v.split() for k, v in zip(dataframe['nombre_doc'],dataframe['raw_clean_text'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean = [*dataframe['raw_clean_text'].str.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#Debemos crear un diccionario de nuestro corpus (lista de docs), donde cada término único sea asignado a un index. \n",
    "\n",
    "#Convirtiendo la lista de documentos corpus en una Matriz de términos de documentos, usando el diccionario dict_data\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(doc_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora correremos el modelo LDA que gensim nos provee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "ldamodel = LDA(doc_term_matrix, num_topics=5, id2word=dictionary, passes=40,random_state=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.012*\"arrendamiento\" + 0.011*\"activo\" + 0.011*\"inversiones\" + 0.010*\"arrendamientos\" + 0.010*\"pasivo\" + 0.010*\"derecho\"'), (1, '0.017*\"arrendamiento\" + 0.013*\"bonos\" + 0.013*\"emision\" + 0.009*\"subsidiarias\" + 0.008*\"vigentes\" + 0.008*\"bolivianos\"'), (2, '0.011*\"instrumentos\" + 0.010*\"inversiones\" + 0.009*\"ingresos\" + 0.009*\"contables\" + 0.009*\"tiempo\" + 0.009*\"cobertura\"'), (3, '0.001*\"inversiones\" + 0.001*\"emision\" + 0.001*\"marzo\" + 0.001*\"enero\" + 0.001*\"serie\" + 0.001*\"contables\"'), (4, '0.001*\"contables\" + 0.001*\"emision\" + 0.001*\"informacion\" + 0.001*\"inversiones\" + 0.001*\"serie\" + 0.001*\"gastos\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=5, num_words=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métricas del modelo: \n",
    "\n",
    "Perplexity: \n",
    "\n",
    "Coherence Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -6.5309116742756395\n",
      "Coherence Score: 0.28504458324472415\n"
     ]
    }
   ],
   "source": [
    "print('Perplexity: {}'.format(ldamodel.log_perplexity(doc_term_matrix))) #Mientras más bajo mejor.\n",
    "\n",
    "#Cohere Score:\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts= doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print('Coherence Score: {}'.format(coherence_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.2686555407995678\n"
     ]
    }
   ],
   "source": [
    "ldamodel2 = LDA(doc_term_matrix, num_topics=3, id2word=dictionary, passes=60,random_state=300)\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel2, texts= doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print('Coherence Score: {}'.format(coherence_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.2597642143164087\n"
     ]
    }
   ],
   "source": [
    "ldamodel2 = LDA(doc_term_matrix, num_topics=7, id2word=dictionary, passes=60,random_state=300)\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel2, texts= doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print('Coherence Score: {}'.format(coherence_lda))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
